<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Collection与Map</title>
    <url>/2023/02/19/java/collection/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2022/07/24/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<span id="more"></span>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>java基础-多线程</title>
    <url>/2023/02/19/java/javaConcurrent1/</url>
    <content><![CDATA[<h1 id="线程的生命周期"><a href="#线程的生命周期" class="headerlink" title="线程的生命周期"></a>线程的生命周期</h1><img src="/2023/02/19/java/javaConcurrent1/threadLife.png" class="" title="img.png">

<h1 id="ThreadLocal"><a href="#ThreadLocal" class="headerlink" title="ThreadLocal"></a>ThreadLocal</h1><p>Thead源码</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">Thread</span> <span class="keyword">implements</span> <span class="title class_">Runnable</span> &#123;</span><br><span class="line">    <span class="comment">//......</span></span><br><span class="line">    <span class="comment">//与此线程有关的ThreadLocal值。由ThreadLocal类维护</span></span><br><span class="line">    ThreadLocal.<span class="type">ThreadLocalMap</span> <span class="variable">threadLocals</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//与此线程有关的InheritableThreadLocal值。由InheritableThreadLocal类维护</span></span><br><span class="line">    ThreadLocal.<span class="type">ThreadLocalMap</span> <span class="variable">inheritableThreadLocals</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">    <span class="comment">//......</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>ThreadLocalMap是TreadLocal的内部类</p>
<img src="/2023/02/19/java/javaConcurrent1/theadLocalMap.png" class="" title="img.png">
<h2 id="ThreadLocal内存泄漏"><a href="#ThreadLocal内存泄漏" class="headerlink" title="ThreadLocal内存泄漏"></a>ThreadLocal内存泄漏</h2><p>ThreadLocalMap 中使用的 key 为 ThreadLocal 的弱引用，而 value 是强引用。所以，如果 ThreadLocal 没有被外部强引用的情况下，在垃圾回收的时候，key 会被清理掉，而 value 不会被清理掉。这样一来，ThreadLocalMap 中就会出现 key 为 null 的 Entry。假如我们不做任何措施的话，value 永远无法被 GC 回收，这个时候就可能会产生内存泄露。ThreadLocalMap 实现中已经考虑了这种情况，在调用 set()、get()、remove() 方法的时候，会清理掉 key 为 null 的记录。使用完 ThreadLocal方法后 最好手动调用remove()方法</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">static</span> <span class="keyword">class</span> <span class="title class_">Entry</span> <span class="keyword">extends</span> <span class="title class_">WeakReference</span>&lt;ThreadLocal&lt;?&gt;&gt; &#123;</span><br><span class="line">    <span class="comment">/** The value associated with this ThreadLocal. */</span></span><br><span class="line">    Object value;</span><br><span class="line"></span><br><span class="line">    Entry(ThreadLocal&lt;?&gt; k, Object v) &#123;</span><br><span class="line">        <span class="built_in">super</span>(k);</span><br><span class="line">        value = v;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h1 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h1><h2 id="volatile"><a href="#volatile" class="headerlink" title="volatile"></a>volatile</h2><blockquote>
<p>保证了变量和可见性和顺序性，无法保证原子性（如自增操作）<br>底层利用内存屏障来实现，Unsafe提供了三个内存屏障的类：<br> <figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title function_">loadFence</span><span class="params">()</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title function_">storeFence</span><span class="params">()</span>;</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">native</span> <span class="keyword">void</span> <span class="title function_">fullFence</span><span class="params">()</span>;</span><br></pre></td></tr></table></figure></p>
</blockquote>
<h2 id="synchronized"><a href="#synchronized" class="headerlink" title="synchronized"></a>synchronized</h2><blockquote>
<ol>
<li>修饰实例方法 （锁当前对象实例）</li>
<li>修饰静态方法 （锁当前类）</li>
<li>修饰代码块 （锁指定对象&#x2F;类）</li>
</ol>
</blockquote>
<blockquote>
<p>synchronized 同步语句块的实现使用的是 monitorenter 和 monitorexit 指令，其中 monitorenter 指令指向同步代码块的开始位置，monitorexit 指令则指明同步代码块的结束位置。synchronized 修饰的方法并没有 monitorenter 指令和 monitorexit 指令，取得代之的确实是 ACC_SYNCHRONIZED 标识，该标识指明了该方法是一个同步方法。不过两者的本质都是对对象监视器 monitor 的获取。</p>
</blockquote>
<h2 id="AQS"><a href="#AQS" class="headerlink" title="AQS"></a>AQS</h2><p>核心内容state和CLH（虚拟的双向队列）<br>自定义同步器实现时主要实现以下几种方法：</p>
<ol>
<li>isHeldExclusively()：该线程是否正在独占资源。只有用到condition才需要去实现它。</li>
<li>tryAcquire(int)：独占方式。尝试获取资源，成功则返回true，失败则返回false。</li>
<li>tryRelease(int)：独占方式。尝试释放资源，成功则返回true，失败则返回false。</li>
<li>tryAcquireShared(int)：共享方式。尝试获取资源。负数表示失败；0表示成功，但没有剩余可用资源；正数表示成功，且有剩余资源。</li>
<li>tryReleaseShared(int)：共享方式。尝试释放资源，如果释放后允许唤醒后续等待结点返回true，否则返回false</li>
</ol>
<h2 id="Semaphore"><a href="#Semaphore" class="headerlink" title="Semaphore"></a>Semaphore</h2><p>限制一定数量的线程执行,内部定义Sync实现AQS，底层使用AQS的state</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">Semaphore sem = new Semaphore(2);</span><br><span class="line">//子线程</span><br><span class="line">sem.acquire();</span><br><span class="line">sem.release();</span><br></pre></td></tr></table></figure>
<h2 id="CountDownLatch"><a href="#CountDownLatch" class="headerlink" title="CountDownLatch"></a>CountDownLatch</h2><p>表示一个线程需要等一批线程执行后才能执行，内部定义Sync实现AQS，底层使用AQS的state，state为一批线程的数量</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">CountDownLatch latch = new CountDownLatch(10);</span><br><span class="line">//子线程</span><br><span class="line">latch.countDown();</span><br><span class="line">//主线程</span><br><span class="line">latch.await();</span><br></pre></td></tr></table></figure>
<h3 id="缺点："><a href="#缺点：" class="headerlink" title="缺点："></a>缺点：</h3><ol>
<li>只能使用一次</li>
</ol>
<h2 id="CyclicBarrier"><a href="#CyclicBarrier" class="headerlink" title="CyclicBarrier"></a>CyclicBarrier</h2><p>表示所有子线程都调用wait后，先执行cyclicBarrier的回调函数，再继续子线程wait后的方法<br>底层依赖ReentrandLock和Condition，利用ReentrantLock的Condition来阻塞和通知线程；</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">CyclicBarrier cyclicBarrier = new CyclicBarrier(10,()-&gt;&#123;</span><br><span class="line">            System.out.println(&quot;所有人都准备好了裁判开始了&quot;);</span><br><span class="line">        &#125;);</span><br><span class="line">//子线程</span><br><span class="line">cyclicBarrier.await();</span><br></pre></td></tr></table></figure>
<h3 id="优点："><a href="#优点：" class="headerlink" title="优点："></a>优点：</h3><ol>
<li>可以循环使用</li>
<li></li>
</ol>
<h2 id="synchronized-和-volatile-有什么区别？"><a href="#synchronized-和-volatile-有什么区别？" class="headerlink" title="synchronized 和 volatile 有什么区别？"></a>synchronized 和 volatile 有什么区别？</h2><h2 id="synchronized-和-ReentrantLock-有什么区别？"><a href="#synchronized-和-ReentrantLock-有什么区别？" class="headerlink" title="synchronized 和 ReentrantLock 有什么区别？"></a>synchronized 和 ReentrantLock 有什么区别？</h2><blockquote>
<ol>
<li>两者都是可重入锁</li>
<li>synchronized 依赖于 JVM 而 ReentrantLock 依赖于 API</li>
<li>ReentrantLock能力更多：等待可中断 ，可实现公平锁，可实现选择性通知（锁可以绑定多个条件）</li>
</ol>
</blockquote>
<h1 id="线程池"><a href="#线程池" class="headerlink" title="线程池"></a>线程池</h1><h2 id="Executors工具类（不推荐）"><a href="#Executors工具类（不推荐）" class="headerlink" title="Executors工具类（不推荐）"></a>Executors工具类（不推荐）</h2><blockquote>
<ol>
<li>FixedThreadPool,允许请求的队列长度为 Integer.MAX_VALUE</li>
<li>SingleThreadExecutor,允许请求的队列长度为 Integer.MAX_VALUE</li>
<li>CachedThreadPool, 允许创建的线程数量为 Integer.MAX_VALUE</li>
<li>ScheduledThreadPool， 允许创建的线程数量为 Integer.MAX_VALUE</li>
</ol>
</blockquote>
<h2 id="ThreadPoolExecutor"><a href="#ThreadPoolExecutor" class="headerlink" title="ThreadPoolExecutor"></a>ThreadPoolExecutor</h2><p><strong>核心参数</strong></p>
<blockquote>
<ol>
<li>corePoolSize：核心线程数</li>
<li>maximumPoolSize：最大线程数，队列中存放的任务达到队列容量的时候才会由新增线程到此数值</li>
<li>workQueue：等待队列</li>
<li>keepAliveTime：存活时间，线程池中的线程数量大于 corePoolSize时，超出的线程的存活时间</li>
<li>unit:存活时间的单位</li>
<li>threadFactory：可以自定义实现，指定线程组和线程名称前缀</li>
<li>handler：饱和策略</li>
</ol>
</blockquote>
<h3 id="workQueue"><a href="#workQueue" class="headerlink" title="workQueue"></a>workQueue</h3><h3 id="handle"><a href="#handle" class="headerlink" title="handle"></a>handle</h3><blockquote>
<ol>
<li>ThreadPoolExecutor.AbortPolicy:抛出异常直接拒绝（默认）</li>
<li>ThreadPoolExecutor.CallerRunsPolicy：调用执行自己的线程运行任务</li>
<li>ThreadPoolExecutor.DiscardPolicy：不处理，直接丢掉</li>
<li>ThreadPoolExecutor.DiscardOldestPolicy：丢弃最早的未处理的任务请求</li>
</ol>
</blockquote>
<img src="/2023/02/19/java/javaConcurrent1/threadpool.png" class="" title="img.png">
<h3 id="线程数数量设置"><a href="#线程数数量设置" class="headerlink" title="线程数数量设置"></a>线程数数量设置</h3><blockquote>
<ol>
<li>CPU 密集型任务(N+1)</li>
<li>IO 密集型任务(2N)</li>
</ol>
</blockquote>
]]></content>
      <categories>
        <category>java</category>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>java类相关简介</title>
    <url>/2023/02/18/java/javaBase1/</url>
    <content><![CDATA[<h1 id="类的加载顺序"><a href="#类的加载顺序" class="headerlink" title="类的加载顺序"></a>类的加载顺序</h1><p>加载—连接（验证、准备、解析）—初始化—使用—卸载</p>
<h1 id="双亲委派"><a href="#双亲委派" class="headerlink" title="双亲委派"></a>双亲委派</h1><img src="/2023/02/18/java/javaBase1/doubleParent.png" class="" title="img.png">
]]></content>
      <categories>
        <category>java</category>
        <category>class</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>jvm简要介绍</title>
    <url>/2023/02/18/java/jvm/</url>
    <content><![CDATA[<h1 id="内存模型"><a href="#内存模型" class="headerlink" title="内存模型"></a>内存模型</h1><img src="/2023/02/18/java/jvm/jvmMoudle.png" class="" title="img.png">

<h2 id="程序计数器"><a href="#程序计数器" class="headerlink" title="程序计数器"></a>程序计数器</h2><ul>
<li>字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制，如：顺序执行、选择、循环、异常处理。</li>
<li>在多线程的情况下，程序计数器用于记录当前线程执行的位置，从而当线程被切换回来的时候能够知道该线程上次运行到哪儿了。</li>
</ul>
<h2 id="虚拟机栈"><a href="#虚拟机栈" class="headerlink" title="虚拟机栈"></a>虚拟机栈</h2><img src="/2023/02/18/java/jvm/stack.png" class="" title="img.png">

<h2 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h2><p>在 JDK 7 版本及 JDK 7 版本之前，堆内存被通常分为下面三部分：</p>
<ul>
<li>新生代内存(Young Generation)</li>
<li>老生代(Old Generation)</li>
<li>永久代(Permanent Generation)<img src="/2023/02/18/java/jvm/heap.png" class="" title="img.png"></li>
</ul>
<h2 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h2><pre><code>方法区会存储已被虚拟机加载的 类信息、字段信息、方法信息、常量、静态变量、即时编译器编译后的代码缓存等数据。  
方法区和永久代以及元空间的关系很像 Java 中接口和类的关系，类实现了接口，这里的类就可以看作是永久代和元空间，接口可以看作是方法区，也就是说永久代以及元空间是 HotSpot 虚拟机对虚拟机规范中方法区的两种实现方式。
</code></pre>
<p><strong>为什么永久代要换成元空间</strong></p>
<ul>
<li>整个永久代有一个 JVM 本身设置的固定大小上限，无法进行调整，而元空间使用的是直接内存，受本机可用内存的限制，虽然元空间仍旧可能溢出，但是比原来出现的几率会更小。使用 -XX：MaxMetaspaceSize 标志设置最大元空间大小，默认值为 unlimited，这意味着它只受系统内存的限制。-XX：MetaspaceSize 调整标志定义元空间的初始大小如果未指定此标志，则 Metaspace 将根据运行时的应用程序需求动态地重新调整大小。</li>
<li>元空间里面存放的是类的元数据，这样加载多少类的元数据就不由 MaxPermSize 控制了, 而由系统的实际可用空间来控制，这样能加载的类就更多了<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">-XX:PermSize=N //方法区 (永久代) 初始大小</span><br><span class="line">-XX:MaxPermSize=N //方法区 (永久代) 最大大小</span><br><span class="line"></span><br><span class="line">-XX:MetaspaceSize=N //设置 Metaspace 的初始（和最小大小）</span><br><span class="line">-XX:MaxMetaspaceSize=N //设置 Metaspace 的最大大小</span><br></pre></td></tr></table></figure>
<img src="/2023/02/18/java/jvm/metaspace.png" class="" title="img.png">
因为永久代（方法区实现）的 GC 回收效率太低，只有在整堆收集 (Full GC)的时候才会被执行 GC。Java 程序中通常会有大量的被创建的字符串等待回收，将字符串常量池放到堆中，能够更高效及时地回收字符串内存。</li>
</ul>
<h1 id="对象创建流程"><a href="#对象创建流程" class="headerlink" title="对象创建流程"></a>对象创建流程</h1><p>  类加载检查-分配内存-初始化-设置对象头-init</p>
<h2 id="分配内存"><a href="#分配内存" class="headerlink" title="分配内存"></a>分配内存</h2><ul>
<li>指针碰撞<blockquote>
<p>用过的内存全部整合到一边，没有用过的内存放在另一边，中间有一个分界指针，只需要向着没用过的内存方向将该指针移动对象内存大小位置即可,代表Serial, ParNew</p>
</blockquote>
</li>
<li>空闲列表<blockquote>
<p>虚拟机会维护一个列表，该列表中会记录哪些内存块是可用的，在分配的时候，找一块儿足够大的内存块儿来划分给对象实例，最后更新列表记录，代表CMS</p>
</blockquote>
</li>
</ul>
<h2 id="内存并发"><a href="#内存并发" class="headerlink" title="内存并发"></a>内存并发</h2><ul>
<li>CAS+失败重试： CAS 是乐观锁的一种实现方式。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。虚拟机采用 CAS 配上失败重试的方式保证更新操作的原子性。</li>
<li>TLAB： 为每一个线程预先在 Eden 区分配一块儿内存，JVM 在给线程中的对象分配内存时，首先在 TLAB 分配，当对象大于 TLAB 中的剩余内存或 TLAB 的内存已用尽时，再采用上述的 CAS 进行内存分配</li>
</ul>
<h1 id="垃圾回收器"><a href="#垃圾回收器" class="headerlink" title="垃圾回收器"></a>垃圾回收器</h1><h2 id="Serial收集器（新生代-复制算法）"><a href="#Serial收集器（新生代-复制算法）" class="headerlink" title="Serial收集器（新生代-复制算法）"></a>Serial收集器（新生代-复制算法）</h2><p>JDK1.3.1之前是虚拟机新生代垃圾回收的唯一选择。这个收集器是一个单线程的，它进行垃圾收集时，其他工作线程会暂停</p>
<h2 id="ParNew收集器（新生代-复制算法）"><a href="#ParNew收集器（新生代-复制算法）" class="headerlink" title="ParNew收集器（新生代-复制算法）"></a>ParNew收集器（新生代-复制算法）</h2><p>Serial收集器的多线程版本，可配合CMS收集器，可以使用-XX:+UseParNewGC强行指定它，或者使用-XX:+UseConcMarkSweepGC选项后的默认新生代收集器，使用-XX:ParallelGCThreads来限制垃圾回收线程的数量</p>
<h2 id="Parallel-Scavenge收集器（新生代-复制算法）"><a href="#Parallel-Scavenge收集器（新生代-复制算法）" class="headerlink" title="Parallel Scavenge收集器（新生代-复制算法）"></a>Parallel Scavenge收集器（新生代-复制算法）</h2><p>采用复制算法，又是并行的多线程垃圾收集器。</p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">-XX:MaxGCPauseMills 最大垃圾手机时间</span><br><span class="line">-XX:GCTimeRatio 设置吞吐量大小</span><br><span class="line">-XX:+UseAdaptiveSizePolicy 自适应</span><br></pre></td></tr></table></figure>
<h2 id="Serial-Old收集器（老年代-标记整理算法）"><a href="#Serial-Old收集器（老年代-标记整理算法）" class="headerlink" title="Serial Old收集器（老年代-标记整理算法）"></a>Serial Old收集器（老年代-标记整理算法）</h2><p>Serial收集器的老年代版本，</p>
<h2 id="Parallel-Old收集器（老年代-多线程和标记整理算法）"><a href="#Parallel-Old收集器（老年代-多线程和标记整理算法）" class="headerlink" title="Parallel Old收集器（老年代-多线程和标记整理算法）"></a>Parallel Old收集器（老年代-多线程和标记整理算法）</h2><p>Parallel Scavenge收集器的老年代版本，</p>
<h2 id="CMS收集器（老年代-标记清除算法）"><a href="#CMS收集器（老年代-标记清除算法）" class="headerlink" title="CMS收集器（老年代-标记清除算法）"></a>CMS收集器（老年代-标记清除算法）</h2><p>Concurrent Mark Sweep，以获取最短停顿时间为目标</p>
<ol>
<li>初始标记 — 仅仅关联GC Roots能直接关联到的对象，速度很快;</li>
<li>并发标记 — 进行GC Roots Tracing的过程;</li>
<li>重新标记 — 为了修正并发标记期间，因用户程序运作而导致标记产生变动的那一部分对象的标记记录;</li>
<li>并发清除</li>
</ol>
<p>初始标记和重新标记需要暂停用户线程，<strong>缺点</strong></p>
<ol>
<li>CMS收集器对CPU资源非常敏感(默认启动的回收线程数是（CPU数量+3）&#x2F;4)</li>
<li>无法处理浮动垃圾(CMS并发清理阶段用户线程还在运行着，伴随程序运行自然就还会有新的垃圾不断产生。这一部分垃圾出现在标记过程之后)</li>
<li>会有大量的垃圾碎片产生 -XX:+UseCMSCompactAtFullCollection</li>
</ol>
<h2 id="G1收集器（整个堆-整理）"><a href="#G1收集器（整个堆-整理）" class="headerlink" title="G1收集器（整个堆-整理）"></a>G1收集器（整个堆-整理）</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//开启G1收集器，设置最大堆内存32G，Gc暂停时间200ms</span></span><br><span class="line">XX:+UseG1GC -Xmx32g -XX:MaxGCPauseMillis=<span class="number">200</span></span><br></pre></td></tr></table></figure>
<p>把Java内存拆分成多等份，多个域（Region）,最多2048个，</p>
<h3 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h3><ol>
<li>分利用多CPU、多核环境下的硬件优势，使用多个CPU来缩短“Stop The World”停顿时间</li>
<li>不会产生碎片</li>
<li>可预测的停顿</li>
</ol>
<p>###回收流程</p>
<ol>
<li>初始标记（Initial Marking），需停止线程，时间很短</li>
<li>并发标记（Concurrent Marking），利用可达性分析，可与用户程序并发执行</li>
<li>修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录，将线程的Remembered Set Logs里面数据合并到Remembered Set（记忆集，跟踪对象引用），需要停顿，但可并行<br>4.筛选回收（Live Data Counting and Evacuation） 首先对各个Region中的回收价值和成本进行排序，需要停顿线程</li>
</ol>
<h3 id="使用G1的情况"><a href="#使用G1的情况" class="headerlink" title="使用G1的情况"></a>使用G1的情况</h3><ul>
<li>实时数据占用超过一半的堆空间</li>
<li>对象分配或者晋升的速度变化大</li>
<li>希望消除长时间的GC停顿（超过0.5-1秒）</li>
</ul>
<h2 id="ZGC"><a href="#ZGC" class="headerlink" title="ZGC"></a>ZGC</h2><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><img src="/2023/02/18/java/jvm/gc.png" class="" title="img.png">
<h1 id="GC日志开启命令"><a href="#GC日志开启命令" class="headerlink" title="GC日志开启命令"></a>GC日志开启命令</h1><figure class="highlight text"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">-XX:+PrintGCDetails 输出GC的详细日志</span><br><span class="line">-XX:+PrintGCDateStamps 输出GC的日期戳</span><br><span class="line">-Xloggc:/var/log/hbase/gc-regionserver-hbase.log GC日志输出的路径</span><br><span class="line">-XX:+UseGCLogFileRotation 打开GC日志滚动记录功能</span><br><span class="line">-XX:NumberOfGCLogFiles 设置滚动日志文件的个数，必须大于等于1  日志文件命名策略是，.0, .1, …, .n-1，其中n是该参数的值</span><br><span class="line">-XX:GCLogFileSize 设置滚动日志文件的大小，必须大于8k</span><br><span class="line">-XX:+PrintGCApplicationStoppedTime 打印GC造成应用暂停的时间</span><br><span class="line">-XX:+PrintHeapAtGC 在进行GC的前后打印出堆的信息</span><br><span class="line">-XX:+PrintTenuringDistribution 在每次新生代 young GC时,输出幸存区中对象的年龄分布</span><br><span class="line"></span><br><span class="line">oom的时候生成dump文件</span><br><span class="line">-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=目录</span><br><span class="line">-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=目录/xxx.hprof</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>总结如下</strong></p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">-XX:+PrintGCDetails</span><br><span class="line">-XX:+PrintGCDateStamps</span><br><span class="line">-Xloggc:D://haha//gc.log</span><br><span class="line">-XX:+UseGCLogFileRotation</span><br><span class="line">-XX:NumberOfGCLogFiles=10</span><br><span class="line">-XX:GCLogFileSize=512k</span><br><span class="line">-XX:+PrintGCApplicationStoppedTime</span><br><span class="line">-XX:+PrintHeapAtGC</span><br><span class="line">-XX:+HeapDumpOnOutOfMemoryError</span><br><span class="line">-XX:HeapDumpPath=D://haha</span><br></pre></td></tr></table></figure>
<h1 id="常见设置参数"><a href="#常见设置参数" class="headerlink" title="常见设置参数"></a>常见设置参数</h1><img src="/2023/02/18/java/jvm/jvmparam.png" class="" title="img.png">

<h1 id="对象结构"><a href="#对象结构" class="headerlink" title="对象结构"></a>对象结构</h1><img src="/2023/02/18/java/jvm/object.png" class="" title="img.png">
<p>以64位操作系统为例，new Object()占用大小分为两种情况：</p>
<ul>
<li>未开启指针压缩</li>
</ul>
<p>占用大小为：8(Mark Word)+8(Class Pointer)&#x3D;16字节</p>
<ul>
<li>开启了指针压缩(默认是开启的)</li>
</ul>
<p>开启指针压缩后，Class Pointer会被压缩为4字节，最终大小为：</p>
<p>8(Mark Word) + 4(Class Pointer) + 4(对齐填充) &#x3D; 16字节</p>
]]></content>
      <categories>
        <category>java</category>
        <category>jvm</category>
      </categories>
      <tags>
        <tag>java</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka概述（二）</title>
    <url>/2022/07/25/kafka/kafka2/</url>
    <content><![CDATA[<h2 id="1-kafka配置详解"><a href="#1-kafka配置详解" class="headerlink" title="1. kafka配置详解"></a>1. kafka配置详解</h2><h3 id="1-1-broker配置"><a href="#1-1-broker配置" class="headerlink" title="1.1 broker配置"></a>1.1 broker配置</h3><blockquote>
<ul>
<li><strong>broker.id</strong>：（必须配置）节点ID，必须全局唯一</li>
<li><strong>log.dir</strong>：（必须配置）日志存放目录，默认值（&#x2F;tmp&#x2F;kafka-logs），tmp目录会被系统定期清理</li>
<li><strong>zookeeper.connect</strong>：（必须配置）zookeeper连接</li>
<li><strong>auto.create.topics.enable</strong>：是否自动创建主题，默认为true</li>
<li><strong>auto.leader.rebalance.enable</strong>：分区leader自动负载均衡，默认true</li>
<li><strong>compression.type</strong>：压缩类型，可选值 (‘gzip’, ‘snappy’, ‘lz4’, ‘zstd’)</li>
</ul>
</blockquote>
<h3 id="1-2-producer配置"><a href="#1-2-producer配置" class="headerlink" title="1.2 producer配置"></a>1.2 producer配置</h3><blockquote>
<ul>
<li><strong>client.id</strong>：生产者客户端Id</li>
<li><strong>batch.size</strong>：批量发送数据大小，默认16K</li>
<li><strong>linger.ms</strong>：延迟时间，默认0</li>
<li><strong>buffer.memory</strong>：缓冲区大小，默认32M</li>
<li><strong>compression.type</strong>：消息压缩格式，默认none，可选none, gzip, snappy, lz4, or zstd</li>
<li><strong>acks</strong>：消息确认机制，默认all，即-1，可选[all, -1, 0, 1]，0-不需要回应，1-等leader落盘后回应，-1-所有副本落盘后回应</li>
<li><strong>partitioner.class</strong>：分区器类路径，默认DefaultPartitioner，还提供RoundRobinPartitioner，UniformStickyPartitioner</li>
<li><strong>key.serializer</strong>：key的序列化器</li>
<li><strong>value.serializer</strong>：value的序列化器</li>
<li><strong>bootstrap.servers</strong>：kafka地址</li>
<li><strong>retries</strong>：重试次数，默认int最大值</li>
<li><strong>enable.idempotence</strong>：幂等性，默认true,根据&lt;pid,分区号，序列号&gt;去重</li>
<li><strong>max.in.flight.requests.per.connection</strong>：生产者在收到kafka响应前最大发送请求数，默认5</li>
<li><strong>transactional.id</strong>：事务ID，全局唯一，基于enable.idempotence</li>
</ul>
</blockquote>
<h3 id="1-3-consume配置"><a href="#1-3-consume配置" class="headerlink" title="1.3 consume配置"></a>1.3 consume配置</h3><blockquote>
<ul>
<li><strong>group.id</strong>：消费者所属的群组ID</li>
<li><strong>enable.auto.commit</strong>：自动提交，默认开启，一般关闭</li>
<li><strong>auto.offset.reset</strong>：有效值为“earliest”“latest”“none”,默认latest</li>
<li><strong>fetch.min.bytes</strong>：最小拉取字节数，默认1(B)，</li>
<li><strong>fetch.max.wait.mss</strong>：拉取最大等待时间数，默认500（ms），</li>
<li><strong>max-poll-records</strong>：一次请求最大拉取的消息条数，默认500</li>
<li><strong>key.serializer</strong>：key的序列化器</li>
<li><strong>value.serializer</strong>：value的序列化器</li>
<li><strong>bootstrap.servers</strong>：kafka地址</li>
</ul>
</blockquote>
<h2 id="2-常用命令"><a href="#2-常用命令" class="headerlink" title="2. 常用命令"></a>2. 常用命令</h2><h3 id="2-1-启动停止"><a href="#2-1-启动停止" class="headerlink" title="2.1 启动停止"></a>2.1 启动停止</h3><blockquote>
<ul>
<li>.&#x2F;bin&#x2F;kafka-server-start.sh -daemon .&#x2F;config&#x2F;server.properties</li>
<li>.&#x2F;bin&#x2F;kafka-server-stop.sh</li>
</ul>
</blockquote>
<h3 id="2-2-主题（kafka-topic-sh）"><a href="#2-2-主题（kafka-topic-sh）" class="headerlink" title="2.2 主题（kafka-topic.sh）"></a>2.2 主题（kafka-topic.sh）</h3><blockquote>
<ul>
<li>–bootstrap-server IP:PORT(多个用逗号分隔)</li>
<li>–topic 主题名称</li>
<li>–create</li>
<li>–delete</li>
<li>–describe</li>
<li>–partitions</li>
<li>–replication-factor</li>
<li>–list</li>
</ul>
</blockquote>
<h3 id="2-3-生产者（kafka-console-producer-sh）"><a href="#2-3-生产者（kafka-console-producer-sh）" class="headerlink" title="2.3 生产者（kafka-console-producer.sh）"></a>2.3 生产者（kafka-console-producer.sh）</h3><blockquote>
<ul>
<li>–bootstrap-server IP:PORT(多个用逗号分隔)</li>
<li>–topic 主题名称</li>
</ul>
</blockquote>
<h3 id="2-4-消费者（kafka-console-consumer-sh）"><a href="#2-4-消费者（kafka-console-consumer-sh）" class="headerlink" title="2.4 消费者（kafka-console-consumer.sh）"></a>2.4 消费者（kafka-console-consumer.sh）</h3><blockquote>
<ul>
<li>–bootstrap-server IP:PORT(多个用逗号分隔)</li>
<li>–topic 主题名称 –from-beginning</li>
<li>–group 指定消费组</li>
</ul>
</blockquote>
]]></content>
      <categories>
        <category>kafka</category>
        <category>基础篇</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka-分区原来是这样子</title>
    <url>/2022/07/30/kafka/kafka3/</url>
    <content><![CDATA[<h1 id="1-分区器"><a href="#1-分区器" class="headerlink" title="1. 分区器"></a>1. 分区器</h1><blockquote>
<p><em><strong>broker配置项：partitioner.class</strong></em></p>
<ul>
<li>org.apache.kafka.clients.producer.internals.DefaultPartitioner</li>
<li>org.apache.kafka.clients.producer.RoundRobinPartitioner</li>
<li>org.apache.kafka.clients.producer.Partitioner</li>
</ul>
</blockquote>
<h2 id="1-1-默认分区器（DefaultPartitioner）"><a href="#1-1-默认分区器（DefaultPartitioner）" class="headerlink" title="1.1 默认分区器（DefaultPartitioner）"></a>1.1 默认分区器（DefaultPartitioner）</h2><blockquote>
<ul>
<li>若发送时指定分区，则发送到指定的分区中</li>
<li>未指定分区，指定了Key，则Key的hashcode%分区数</li>
<li>均未指定，采取粘性规则，第一次随机选择分区，直到缓存满或者时间到发送完消息，下一次继续随机但不会选择上次使用的分区</li>
</ul>
</blockquote>
<h2 id="1-2-RoundRobinPartitioner"><a href="#1-2-RoundRobinPartitioner" class="headerlink" title="1.2 RoundRobinPartitioner"></a>1.2 RoundRobinPartitioner</h2><blockquote>
<p>这种分区策略是一系列连续记录中的每条记录将被发送到不同的分区（无论是否提供’key’），直到我们用完分区并重新开始。注意：有一个已知问题会在创建新批次时导致分布不均</p>
</blockquote>
<h2 id="1-3-UniformStickyPartitioner"><a href="#1-3-UniformStickyPartitioner" class="headerlink" title="1.3 UniformStickyPartitioner"></a>1.3 UniformStickyPartitioner</h2><blockquote>
<p>此分区策略将尝试坚持一个分区（无论是否提供了“key”），直到batch.size已满或已满linger.ms</p>
</blockquote>
<h2 id="1-4-自定义分区"><a href="#1-4-自定义分区" class="headerlink" title="1.4 自定义分区"></a>1.4 自定义分区</h2><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.producer.Partitioner;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.Cluster;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.PartitionInfo;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"><span class="keyword">import</span> java.util.Map;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 自定义hash分区,实现Partitioner接口,重写partition方法</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyPartition</span> <span class="keyword">implements</span> <span class="title class_">Partitioner</span> &#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">partition</span><span class="params">(String topic, Object key, <span class="type">byte</span>[] keyBytes, Object value, <span class="type">byte</span>[] valueBytes1, Cluster cluster)</span> &#123;</span><br><span class="line">        <span class="comment">// 获取topic中partition数量</span></span><br><span class="line">        List&lt;PartitionInfo&gt; partitionInfoList = cluster.availablePartitionsForTopic(topic);</span><br><span class="line">        <span class="type">int</span> <span class="variable">partitionCount</span> <span class="operator">=</span> partitionInfoList.size();</span><br><span class="line">        <span class="comment">// 根据key的hash值计取模，计算出在哪个分区中</span></span><br><span class="line">        <span class="type">int</span> <span class="variable">numPartitions</span> <span class="operator">=</span> Math.abs(String.valueOf(key).hashCode()) % partitionCount;</span><br><span class="line">        <span class="keyword">return</span> numPartitions;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">close</span><span class="params">()</span> &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">configure</span><span class="params">(Map&lt;String, ?&gt; map)</span> &#123;</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">kafkaProperties.put(<span class="string">&quot;partitioner.class&quot;</span>,<span class="string">&quot;com.pg.kafka.MyPartition&quot;</span>);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="2-分区节点分布策略"><a href="#2-分区节点分布策略" class="headerlink" title="2. 分区节点分布策略"></a>2. 分区节点分布策略</h1><p>创建topic,分配时，规则是尽量均匀将所有分区副本分布在各个broker上</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./kafka-topics.sh --bootstrap-server 127.0.0.1:9092 --create --replication-factor 3  --partitions 16 --topic test2</span><br></pre></td></tr></table></figure>
<p>查看topic信息</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./kafka-topics.sh --bootstrap-server 127.0.0.1:9092 --describe  --topic test2</span><br></pre></td></tr></table></figure>
<img src="/2022/07/30/kafka/kafka3/topic-describe.png" class="" title="img.png">
<blockquote>
<p>分配规则大致如下，借鉴网上教程的图</p>
</blockquote>
<img src="/2022/07/30/kafka/kafka3/partitions-divide.png" class="" title="img.png">
<h1 id="3-分区消费者分配策略"><a href="#3-分区消费者分配策略" class="headerlink" title="3. 分区消费者分配策略"></a>3. 分区消费者分配策略</h1><p><em><strong>消费者配置项：partition.assignment.strategy</strong></em></p>
<h2 id="3-1-org-apache-kafka-clients-consumer-RangeAssignor"><a href="#3-1-org-apache-kafka-clients-consumer-RangeAssignor" class="headerlink" title="3.1 org.apache.kafka.clients.consumer.RangeAssignor"></a>3.1 org.apache.kafka.clients.consumer.RangeAssignor</h2><blockquote>
<p>以Topic为基础，将分区以分区号排序均匀分布给每个消费组中的消费者（按字母顺序排序），比如topic有7个分区，消费组中有3个消费者，<br>他会将分区数与消费者整除，余出的会加在前面的消费者身上，分区如下</p>
<ul>
<li>1号消费者：0,1,2</li>
<li>2号消费者：3,4</li>
<li>3号消费者：5,6</li>
</ul>
<p>这样问题就来了，当同一消费组订阅Topic过多，且不能整除的时候，前面的消费者会承担更多的分区消费，容易产生数据倾斜<br><strong>且这种策略，当一个消费者挂了之后，原属于该消费者的分区的消费任务会全部加到另一个分区上去，消费完成，后续的才会触发在再平衡</strong></p>
</blockquote>
<h2 id="3-2-org-apache-kafka-clients-consumer-RoundRobinAssignor"><a href="#3-2-org-apache-kafka-clients-consumer-RoundRobinAssignor" class="headerlink" title="3.2 org.apache.kafka.clients.consumer.RoundRobinAssignor"></a>3.2 org.apache.kafka.clients.consumer.RoundRobinAssignor</h2><blockquote>
<p>将一个消费组中的所有订阅的topic的分区汇在一起，按照消费者进行轮询分配，当消费组内所有消费者订阅Topic相同时，则这种分配时均匀的，如下：<br>消费者1，消费者2均订阅Topic1，Topic2，俩个Topic均有3个分区，则分配如下：</p>
<ul>
<li>消费者1：T1P0 T1P2 T2P1</li>
<li>消费者2：T1P1 T2P0 T2P2</li>
</ul>
<p>但当组内消费者订阅Topic不相同的时候，也会造成分配不均匀，例如：<br>消费者1，消费者2均订阅Topic1，Topic2，俩个Topic均有1个分区，且消费者2还订阅Topic3，有俩个分区则分配如下：</p>
<ul>
<li>消费者1：T1P0  </li>
<li>消费者2：T2P0 T3P0 T3P1</li>
</ul>
</blockquote>
<h2 id="3-3-org-apache-kafka-clients-consumer-StickyAssignor"><a href="#3-3-org-apache-kafka-clients-consumer-StickyAssignor" class="headerlink" title="3.3 org.apache.kafka.clients.consumer.StickyAssignor"></a>3.3 org.apache.kafka.clients.consumer.StickyAssignor</h2><blockquote>
<p>本策略有两个目标， 首先是要实现分区分配要尽可能地均匀，其次当发生分区再平衡发生时，分区的分配会尽可能的与上次的分配结果保持一致，目的是为了防止<br>分区的消费者发生变化，这有助于节约开销，也有助于避免消息重复消费的问题发生。需要注意的是，当以上两点发生冲突的时候，第一个目标是优先于第二个目标的,例如：<br>三个消费者C1，C2,C3,订阅了三个主题，且每个主题2个分区，</p>
<ul>
<li>C1：T1P0 T2P1</li>
<li>C2：T1P1 T3P0</li>
<li>C3：T2P0 T3P1  不一定按照这个排序哈</li>
</ul>
<p>当订阅不同时，例如<br>三个消费者，三个topic，分别有1,2,3个分区，消费者C1订阅了主题T0，消费者C2订阅了主题T0、T1，消费者C3订阅了主题T0、T1、T2分配如下</p>
<ul>
<li>C1:T0P0</li>
<li>C2:T1P0 T1P1</li>
<li>C3:T2P0 T2P1 T2P2</li>
</ul>
<p>当C1挂掉的时候会再平衡为</p>
<ul>
<li>C2:T1P0 T1P1 T0P0</li>
<li>C3:T2P0 T2P1 T2P2</li>
</ul>
<p><strong>RoundRobinAssignor和StickyAssignor非常重的要区别</strong></p>
<ul>
<li>StickyAssignor在消费组中每个消费者订阅不同topic时，能够使分配更加均匀</li>
<li>StickyAssignor某个消费者宕机后，再平衡时能够保留上次的，分配结果，只对宕机上的分区进行再分配，而RoundRobinAssignor不能保证，<br>  比如说，C1,C2,C3,在C1下线后，会将所有分区轮询C2，C3进行重新分配</li>
</ul>
</blockquote>
<h2 id="3-4-org-apache-kafka-clients-consumer-CooperativeStickyAssignor"><a href="#3-4-org-apache-kafka-clients-consumer-CooperativeStickyAssignor" class="headerlink" title="3.4 org.apache.kafka.clients.consumer.CooperativeStickyAssignor"></a>3.4 org.apache.kafka.clients.consumer.CooperativeStickyAssignor</h2><h2 id="3-5-自定义Assignor"><a href="#3-5-自定义Assignor" class="headerlink" title="3.5 自定义Assignor"></a>3.5 自定义Assignor</h2><blockquote>
<p>实现org.apache.kafka.clients.consumer.ConsumerPartitionAssignor接口</p>
</blockquote>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.clients.consumer.ConsumerPartitionAssignor;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.Cluster;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">MyConsumerPartitionAssignor</span> <span class="keyword">implements</span> <span class="title class_">ConsumerPartitionAssignor</span> &#123;</span><br><span class="line">    </span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> GroupAssignment <span class="title function_">assign</span><span class="params">(Cluster metadata, GroupSubscription groupSubscription)</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="keyword">public</span> String <span class="title function_">name</span><span class="params">()</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="3-6-消费者分区分配规则流程"><a href="#3-6-消费者分区分配规则流程" class="headerlink" title="3.6 消费者分区分配规则流程"></a>3.6 消费者分区分配规则流程</h2><img src="/2022/07/30/kafka/kafka3/consumer-partition-assignment.png" class="" title="img.png">


]]></content>
      <categories>
        <category>kafka</category>
        <category>进阶篇</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka概述（一）</title>
    <url>/2022/07/24/kafka/kafka1/</url>
    <content><![CDATA[<h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><h2 id="1、定义"><a href="#1、定义" class="headerlink" title="1、定义"></a>1、定义</h2><blockquote>
<p>Kafka 是由 Linkedin 公司开发的， 是一个分布式的基于发布&#x2F;订阅模式的消息队列（Message Queue）,同时是支持多分区、多副本的分布式消息流平台，适合大数据的存储以及计算</p>
</blockquote>
<h2 id="2、应用场景"><a href="#2、应用场景" class="headerlink" title="2、应用场景"></a>2、应用场景</h2><blockquote>
<p><strong>限流削峰</strong>：在非常高的并发下，防止导致服务系统崩溃，消息队列能帮忙服务顶住突发的访问压力，解决生产能力和消费能力不一致的问题</p>
</blockquote>
<blockquote>
<p><strong>服务解耦</strong>：解除不同服务之间的依赖关系，防止一端服务变更导致另一端服务需要同步变更或崩溃的问题，使修改更为灵活，维护与开发成本降低</p>
</blockquote>
<blockquote>
<p><strong>异步通信</strong>：允许将消息放入队列中不用立即处理，且生产者发送消息时也可选择异步或者同步发送</p>
</blockquote>
<h2 id="3、特点"><a href="#3、特点" class="headerlink" title="3、特点"></a>3、特点</h2><blockquote>
<p><strong>高吞吐、低延迟</strong>：kakfa 最大的特点就是收发消息非常快，kafka 每秒可以处理几十万条消息，它的最低延迟只有几毫秒；<br><strong>高伸缩性</strong>：每个主题(topic) 包含多个分区(partition)，主题中的分区可以分布在不同的主机(broker)中；<br><strong>持久性、可靠性</strong>：Kafka 能够允许数据的持久化存储，消息被持久化到磁盘，并支持数据备份防止数据丢失，Kafka 底层的数据存储是基于 Zookeeper 存储的，<br>Zookeeper 我们知道它的数据能够持久存储；<br><strong>容错性</strong>：允许集群中的节点失败，某个节点宕机，Kafka 集群能够正常工作；<br><strong>高并发</strong>：支持数千个客户端同时读写。</p>
</blockquote>
<h2 id="4、基本概念"><a href="#4、基本概念" class="headerlink" title="4、基本概念"></a>4、基本概念</h2><blockquote>
<p><strong>生产者（Producer）</strong>：向 Kafka 发布（写入）事件的客户端应用程序<br><strong>消费者（Consumer）</strong>：订阅（读取和处理）事件的客户端应用程序<br><strong>节点（Broker）</strong>：kafka所安装的服务器，负责存储和读取消息<br><strong>主题（Topic）</strong>：消息的主题，每条发布到队列中的消息都隶属一个主题<br><strong>分区（Partition）</strong>：可以为主题划定分区，存储在不同的节点，提高消息存储以及读取的速率<br><strong>消费者群组（Consumer Group）</strong>：包含一组消费者，topic的一个分区只会被同一消费组中的某一个消费者进行消费</p>
</blockquote>
<h2 id="5、对比"><a href="#5、对比" class="headerlink" title="5、对比"></a>5、对比</h2><img src="/2022/07/24/kafka/kafka1/kafka-compare.png" class="" title="img.png">  



]]></content>
      <categories>
        <category>kafka</category>
        <category>基础篇</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>redis</title>
    <url>/2023/02/19/redis/redisDataType/</url>
    <content><![CDATA[<h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><h2 id="String"><a href="#String" class="headerlink" title="String"></a>String</h2><p>其底层实现就可以分为 int、embstr 以及 raw 这三种类型</p>
<p>Redis 中所有的 key 都是字符串，这些字符串是通过一个名为简单动态字符串（SDS） 的抽象数据类型实现的</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">sdshdr</span>&#123;</span></span><br><span class="line">     <span class="comment">//记录buf数组中已使用字节的数量</span></span><br><span class="line">     <span class="comment">//等于 SDS 保存字符串的长度</span></span><br><span class="line">     <span class="type">int</span> len;</span><br><span class="line">     <span class="comment">//记录 buf 数组中未使用字节的数量</span></span><br><span class="line">     <span class="type">int</span> <span class="built_in">free</span>;</span><br><span class="line">     <span class="comment">//字节数组，用于保存字符串</span></span><br><span class="line">     <span class="type">char</span> buf[];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>1）如果一个字符串对象保存的是整数值，并且这个整数值可以用 long 类型标识，那么字符串对象会讲整数值保存在 ptr 属性中，并将 encoding 设置为 int。比如 set number 10086 命令。</p>
<p>2）如果字符串对象保存的是一个字符串值，并且这个字符串的长度大于 44 字节，那么字符串对象将使用一个简单动态字符串（SDS）来保存这个字符串值，并将对象的编码设置为 raw。</p>
<p>3）如果字符串对象保存的是一个字符串值，并且这个字符串的长度小于等于 44 字节，那么字符串对象将使用 embstr 编码的方式来保存这个字符串。</p>
<p>存储形式是这样一种存储形式，它将 RedisObject 对象头和 SDS 对象连续存在一起，使用 malloc 方法一次分配。embstr的最小占用空间为19（16+3），而64-19-1（结尾的\0）&#x3D;44，所以empstr只能容纳44字节</p>
<h2 id="List"><a href="#List" class="headerlink" title="List"></a>List</h2><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">list</span>&#123;</span></span><br><span class="line">     <span class="comment">//表头节点</span></span><br><span class="line">     listNode *head;</span><br><span class="line">     <span class="comment">//表尾节点</span></span><br><span class="line">     listNode *tail;</span><br><span class="line">     <span class="comment">//链表所包含的节点数量</span></span><br><span class="line">     <span class="type">unsigned</span> <span class="type">long</span> len;</span><br><span class="line">     <span class="comment">//节点值复制函数</span></span><br><span class="line">     <span class="type">void</span> (*<span class="built_in">free</span>) (<span class="type">void</span> *ptr);</span><br><span class="line">     <span class="comment">//节点值释放函数</span></span><br><span class="line">     <span class="type">void</span> (*<span class="built_in">free</span>) (<span class="type">void</span> *ptr);</span><br><span class="line">     <span class="comment">//节点值对比函数</span></span><br><span class="line">     <span class="type">int</span> (*match) (<span class="type">void</span> *ptr,<span class="type">void</span> *key);</span><br><span class="line">&#125;<span class="built_in">list</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span>  <span class="class"><span class="keyword">struct</span> <span class="title">listNode</span>&#123;</span></span><br><span class="line">       <span class="comment">//前置节点</span></span><br><span class="line">       <span class="class"><span class="keyword">struct</span> <span class="title">listNode</span> *<span class="title">prev</span>;</span></span><br><span class="line">       <span class="comment">//后置节点</span></span><br><span class="line">       <span class="class"><span class="keyword">struct</span> <span class="title">listNode</span> *<span class="title">next</span>;</span></span><br><span class="line">       <span class="comment">//节点的值</span></span><br><span class="line">       <span class="type">void</span> *value;  </span><br><span class="line">&#125;listNode</span><br></pre></td></tr></table></figure>
<ol>
<li>双端：链表具有前置节点和后置节点的引用，获取这两个节点时间复杂度都为 O(1)。</li>
<li>无环：表头节点的 prev 指针和表尾节点的 next 指针都指向 NULL,对链表的访问都是以 NULL 结束。</li>
<li>带长度计数器：通过 len 属性获取链表长度的时间复杂度为 O(1)。</li>
<li>多态：链表节点使用指针来保存节点值，可以保存各种不同类型的值。</li>
</ol>
<h2 id="SET"><a href="#SET" class="headerlink" title="SET"></a>SET</h2><p>Set底层实现分为俩种，hashtable和inset</p>
<h3 id="hashtable"><a href="#hashtable" class="headerlink" title="hashtable:"></a>hashtable:</h3><p>key为Set的值，value为null</p>
<h3 id="inset"><a href="#inset" class="headerlink" title="inset"></a>inset</h3><p>简单理解为数组</p>
<h4 id="使用条件"><a href="#使用条件" class="headerlink" title="使用条件"></a>使用条件</h4><ol>
<li>元素个数不少于默认值512</li>
<li>元素可以用整型表示</li>
</ol>
<h2 id="ZSET"><a href="#ZSET" class="headerlink" title="ZSET"></a>ZSET</h2><p>底层实现为 字典（dict） + 跳表（skiplist），当数据比较少的时候用ziplist编码结构存储</p>
<h3 id="ziplist"><a href="#ziplist" class="headerlink" title="ziplist"></a>ziplist</h3><p>ziplist作为zset的底层存储结构时候，每个集合元素使用两个紧挨在一起的压缩列表节点来保存，第一个节点保存元素的成员，第二个元素保存元素的分值</p>
<img src="/2023/02/19/redis/redisDataType/ziplist.png" class="" title="img.png">
<h4 id="使用条件-1"><a href="#使用条件-1" class="headerlink" title="使用条件"></a>使用条件</h4><ol>
<li>有序集合保存的元素数量小于默认值128个</li>
<li>有序集合保存的所有元素的长度小于默认值64字节</li>
</ol>
<h3 id="skipList"><a href="#skipList" class="headerlink" title="skipList"></a>skipList</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">zset</span> &#123;</span></span><br><span class="line">    zskiplist *zs1;</span><br><span class="line">    dict *dict;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">![img.png](img.png)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>redis</category>
        <category>数据结构</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>redis-面试题</title>
    <url>/2023/02/20/redis/redisMianShi/</url>
    <content><![CDATA[<h1 id="一、基本概念"><a href="#一、基本概念" class="headerlink" title="一、基本概念"></a>一、基本概念</h1><h3 id="Redis为什么这么快"><a href="#Redis为什么这么快" class="headerlink" title="Redis为什么这么快"></a>Redis为什么这么快</h3><ol>
<li>基于内存</li>
<li>单线程，IO多路复用</li>
<li>高级数据结构（如 SDS、Hash以及跳表等）</li>
</ol>
<h3 id="缓存三大问题以及解决方案？"><a href="#缓存三大问题以及解决方案？" class="headerlink" title="缓存三大问题以及解决方案？"></a>缓存三大问题以及解决方案？</h3><h4 id="缓存穿透（查询数据不存在）"><a href="#缓存穿透（查询数据不存在）" class="headerlink" title="缓存穿透（查询数据不存在）"></a>缓存穿透（查询数据不存在）</h4><p><strong>解决办法</strong></p>
<blockquote>
<ol>
<li>缓存空值</li>
<li>布隆过滤器做Key值校验</li>
</ol>
</blockquote>
<h4 id="缓存击穿（缓存过期，伴随大量对该-key-的请求）"><a href="#缓存击穿（缓存过期，伴随大量对该-key-的请求）" class="headerlink" title="缓存击穿（缓存过期，伴随大量对该 key 的请求）"></a>缓存击穿（缓存过期，伴随大量对该 key 的请求）</h4><p><strong>解决办法</strong></p>
<blockquote>
<ol>
<li>互斥锁</li>
<li>热点数据永不过期</li>
<li>熔断降级</li>
</ol>
</blockquote>
<h4 id="缓存雪崩（同一时间大批量的-key-过期）"><a href="#缓存雪崩（同一时间大批量的-key-过期）" class="headerlink" title="缓存雪崩（同一时间大批量的 key 过期）"></a>缓存雪崩（同一时间大批量的 key 过期）</h4><p><strong>解决办法</strong></p>
<blockquote>
<ol>
<li>随机分散过期时间</li>
<li>热点数据永不过期</li>
</ol>
</blockquote>
<h3 id="一致性解决办法"><a href="#一致性解决办法" class="headerlink" title="一致性解决办法"></a>一致性解决办法</h3><h4 id="强一致性：串行化"><a href="#强一致性：串行化" class="headerlink" title="强一致性：串行化"></a>强一致性：串行化</h4><h4 id="弱一致性：延时双删"><a href="#弱一致性：延时双删" class="headerlink" title="弱一致性：延时双删"></a>弱一致性：延时双删</h4><h3 id="保证redis的高并发"><a href="#保证redis的高并发" class="headerlink" title="保证redis的高并发"></a>保证redis的高并发</h3><p> 主从加集群，读写分离</p>
<h3 id="保证原子性"><a href="#保证原子性" class="headerlink" title="保证原子性"></a>保证原子性</h3><ol>
<li>使用 incr、decr、setnx 等原子操作</li>
<li>使用锁</li>
<li>使用lua脚本</li>
</ol>
<h3 id="Redis-是如何实现字典的？"><a href="#Redis-是如何实现字典的？" class="headerlink" title="Redis 是如何实现字典的？"></a>Redis 是如何实现字典的？</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">dictEntry</span> &#123;</span></span><br><span class="line">    <span class="type">void</span> *key;                <span class="comment">//键</span></span><br><span class="line">    <span class="class"><span class="keyword">union</span> &#123;</span></span><br><span class="line">        <span class="type">void</span> *val;            <span class="comment">//值</span></span><br><span class="line">        <span class="type">uint64_t</span> u64;</span><br><span class="line">        <span class="type">int64_t</span> s64;</span><br><span class="line">        <span class="type">double</span> d;</span><br><span class="line">    &#125; v;</span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">dictEntry</span> *<span class="title">next</span>;</span> <span class="comment">//指向下一个节点，形成链表</span></span><br><span class="line">&#125; dictEntry;</span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">dictht</span>&#123;</span></span><br><span class="line">     <span class="comment">//哈希表数组</span></span><br><span class="line">     dictEntry **table;</span><br><span class="line">     <span class="comment">//哈希表大小</span></span><br><span class="line">     <span class="type">unsigned</span> <span class="type">long</span> size;</span><br><span class="line">     <span class="comment">//哈希表大小掩码，用于计算索引值</span></span><br><span class="line">     <span class="comment">//总是等于 size-1</span></span><br><span class="line">     <span class="type">unsigned</span> <span class="type">long</span> sizemask;</span><br><span class="line">     <span class="comment">//该哈希表已有节点的数量</span></span><br><span class="line">     <span class="type">unsigned</span> <span class="type">long</span> used;</span><br><span class="line"> </span><br><span class="line">&#125;dictht</span><br><span class="line"></span><br><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">dict</span> &#123;</span></span><br><span class="line">    dictType *type;</span><br><span class="line">    <span class="type">void</span> *privdata;</span><br><span class="line">    dictht ht[<span class="number">2</span>];<span class="comment">// 字典只使用 ht[0] 哈希表， ht[1] 哈希表只会在对 ht[0] 哈希表进行 rehash 时使用。</span></span><br><span class="line">    <span class="type">long</span> rehashidx; <span class="comment">/*记录了 rehash 目前的进度 rehashing not in progress if rehashidx == -1 */</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">long</span> iterators; <span class="comment">/* number of iterators currently running */</span></span><br><span class="line">&#125; dict;</span><br></pre></td></tr></table></figure>

<h3 id="渐进式Hash"><a href="#渐进式Hash" class="headerlink" title="渐进式Hash"></a>渐进式Hash</h3><h4 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h4><p><strong>满足任一条件</strong></p>
<ul>
<li>服务器目前没有在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 1</li>
<li>服务器目前正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令， 并且哈希表的负载因子大于等于 5</li>
</ul>
<h4 id="缩容"><a href="#缩容" class="headerlink" title="缩容"></a>缩容</h4><ul>
<li>正在执行 BGSAVE 命令或者 BGREWRITEAOF 命令，则不进行缩容</li>
<li>当哈希表的负载因子小于 0.1 时， 程序自动开始对哈希表执行收缩操作。</li>
</ul>
<h4 id="步骤："><a href="#步骤：" class="headerlink" title="步骤："></a>步骤：</h4><ol>
<li>为 ht[1] 分配空间， 让字典同时持有 ht[0] 和 ht[1] 两个哈希表</li>
<li>字典中维持一个索引计数器变量 rehashidx ， 并将它的值设置为 0 ， 表示 rehash 工作正式开始</li>
<li>rehash 进行期间， 每次对字典执行添加、删除、查找或者更新操作时， 程序除了执行指定的操作以外， 还会顺带将 ht[0] 哈希表在 rehashidx 索引上的所有键值对 rehash 到 ht[1] ， 当 rehash 工作完成之后， 程序将 rehashidx 属性的值增一</li>
<li>redis周期函数中，如果发现有字典正在进行渐进式rehash操作，则会花费1毫秒的时间，帮助一起进行渐进式rehash操作</li>
<li>ht[0] 的所有键值对都会被 rehash 至 ht[1] ， 这时程序将 rehashidx 属性的值设为 -1 ， 表示 rehash 操作已完成</li>
</ol>
<h4 id="影响："><a href="#影响：" class="headerlink" title="影响："></a>影响：</h4><ol>
<li>读取、删除、更新会同时在俩张表上进行，先ht[0],后ht[1]</li>
<li>新增只会在ht[1]</li>
<li>在rehash期间，同时有两个hash表在使用，会使得redis内存使用量瞬间突增，在Redis 满容状态下由于Rehash会导致大量Key驱逐</li>
</ol>
<h4 id="拓展：redis的周期函数有哪些"><a href="#拓展：redis的周期函数有哪些" class="headerlink" title="拓展：redis的周期函数有哪些"></a>拓展：redis的周期函数有哪些</h4><h3 id="Zset-为何不使用红黑树等平衡树？"><a href="#Zset-为何不使用红黑树等平衡树？" class="headerlink" title="Zset 为何不使用红黑树等平衡树？"></a>Zset 为何不使用红黑树等平衡树？</h3><ol>
<li>跳跃表范围查询比平衡树操作简单</li>
<li>平衡树的删除和插入需要对子树进行相应的调整，而跳表只需要修改相邻的节点即可</li>
<li>跳表和平衡树的查询操作都是O（logN）的时间复杂度</li>
</ol>
<h3 id="什么是-RedisObject？"><a href="#什么是-RedisObject？" class="headerlink" title="什么是 RedisObject？"></a>什么是 RedisObject？</h3><p>也就是我们常说的五种数据结构：字符串对象、列表对象、哈希对象、集合对象和有序集合对象等<br><strong>这样做有两个好处</strong>：<br>1）通过不同类型的对象，Redis 可以在执行命令之前，根据对象的类型来判断一个对象是否可以执行该的命令。<br>2）可以针对不同的使用场景，为对象设置不同的实现，从而优化内存或查询速度。</p>
<h3 id="Redis过期策略"><a href="#Redis过期策略" class="headerlink" title="Redis过期策略"></a>Redis过期策略</h3><p>采用惰性+定期删除策略，memcached只用了惰性删除</p>
<h4 id="惰性删除"><a href="#惰性删除" class="headerlink" title="惰性删除"></a>惰性删除</h4><ol>
<li>进行get或setnx等操作时，先检查key是否过期，过期则删除</li>
</ol>
<h4 id="定期删除"><a href="#定期删除" class="headerlink" title="定期删除"></a>定期删除</h4><p>在配置文件中根据server.hz来配置，默认10，即每秒10次</p>
<ol>
<li>遍历每个数据库，检查指定个key</li>
</ol>
<h3 id="持久化文件对过期策略的处理？"><a href="#持久化文件对过期策略的处理？" class="headerlink" title="持久化文件对过期策略的处理？"></a>持久化文件对过期策略的处理？</h3><h4 id="RDB："><a href="#RDB：" class="headerlink" title="RDB："></a>RDB：</h4><ol>
<li>持久化时检查是否过期，过期不持久化</li>
<li>恢复时同理判断</li>
</ol>
<h4 id="AOF"><a href="#AOF" class="headerlink" title="AOF"></a>AOF</h4><ol>
<li>当key过期后，还没有被删除，此时进行执行持久化操作（该key是不会进入aof文件的，因为没有发生修改命令）</li>
<li>当key过期后，在发生删除操作时，程序会向aof文件追加一条del命令（在将来的以aof文件恢复数据的时候该过期的键就会被删掉）</li>
<li>重写时，会先判断key是否过期，已过期的key不会重写到aof文件</li>
</ol>
<h3 id="Redis-有哪些内存淘汰机制？"><a href="#Redis-有哪些内存淘汰机制？" class="headerlink" title="Redis 有哪些内存淘汰机制？"></a>Redis 有哪些内存淘汰机制？</h3><h4 id="volatile"><a href="#volatile" class="headerlink" title="volatile"></a>volatile</h4><ol>
<li>volatile-lru：设置过期时间且最近最少使用淘汰</li>
<li>volatile-ttl：设置过期时间且将要过期的淘汰</li>
<li>volatile-random：设置过期时间的随机淘汰</li>
<li>volatile-lfu：设置过期时间的且使用频率最低淘汰</li>
</ol>
<h4 id="allKeys"><a href="#allKeys" class="headerlink" title="allKeys"></a>allKeys</h4><ol>
<li>allkeys-lru：最近最少使用</li>
<li>allkeys-lfu: 使用频率最低</li>
<li>allkeys-random： 随机</li>
</ol>
<h4 id="no-enviction"><a href="#no-enviction" class="headerlink" title="no-enviction"></a>no-enviction</h4><p>禁止驱逐数据，默认策略</p>
<h3 id="Redis-有哪些持久化机制？"><a href="#Redis-有哪些持久化机制？" class="headerlink" title="Redis 有哪些持久化机制？"></a>Redis 有哪些持久化机制？</h3><h4 id="RDB"><a href="#RDB" class="headerlink" title="RDB"></a>RDB</h4><p>在指定的时间间隔内将内存中的数据集快照写入磁盘，默认的文件名为 dump.rdb，支持 同步（save 命令）、后台异步（bgsave）以及自动配置三种方式触发<br><strong>优点：</strong>  </p>
<ol>
<li>文件紧凑，全量备份，非常适合用于进行备份和灾难恢复</li>
<li>支持异步</li>
<li>恢复大数据集时比AOF快</li>
</ol>
<p><strong>缺点：</strong></p>
<ol>
<li>在快照持久化期间修改的数据不会被保存，可能丢失数据</li>
</ol>
<h4 id="AOF-1"><a href="#AOF-1" class="headerlink" title="AOF"></a>AOF</h4><p>将每一个收到的写命令追加到文件中，三种触发方式：1）每修改同步 always 2）每秒同步 everysec 3）不同no：从不同步<br><strong>优点</strong></p>
<ol>
<li>更好的保护数据不丢失</li>
<li>通过非常可读的方式进行记录，这个特性非常适合做灾难性的误删除的紧急恢复</li>
<li>没有任何磁盘寻址的开销，写入性能非常高，文件不容易破损</li>
<li>不会影响客户端的读写</li>
</ol>
<p><strong>缺点：</strong>  </p>
<ol>
<li>AOF 日志文件通常比 RDB 数据快照文件更大</li>
<li>支持的写 QPS 会比RDB支持的写 QPS 低</li>
</ol>
<h4 id="结合使用"><a href="#结合使用" class="headerlink" title="结合使用"></a>结合使用</h4><p>Redis从4.0版本开始引入RDB-AOF混合持久化模式，这种模式是基于AOF持久化模式构建而来，打开了服务器的AOF持久化功能，并且将aof-use-rdb-preamble <value>选项的值设置成了yes。<br><strong>原理：</strong><br>Redis服务器 在执行 AOF重写操作时，就会像执行BGSAVE命令那样，根据数据库当前的状态 生成出 相应的RDB数据，并将这些数据 写入 新建的AOF文件中，至于那些 在AOF重写开始之后 执行的Redis命令，则会继续以协议文本的方式 追加到 新AOF文件的末尾，即已有的RDB数据的后面</p>
<img src="/2023/02/20/redis/redisMianShi/RdbAndAof.png" class="" title="img.png">
<h3 id="redis单线程IO多路复用"><a href="#redis单线程IO多路复用" class="headerlink" title="redis单线程IO多路复用"></a>redis单线程IO多路复用</h3><img src="/2023/02/20/redis/redisMianShi/multiThread.png" class="" title="img.png">
<ol>
<li>IO 线程要么同时在读 socket，要么同时在写，不会同时读或写</li>
<li>IO 线程只负责读写 socket 解析命令，不负责命令处理。</li>
</ol>
<h1 id="集群"><a href="#集群" class="headerlink" title="集群"></a>集群</h1><h3 id="Redis主从复制"><a href="#Redis主从复制" class="headerlink" title="Redis主从复制"></a>Redis主从复制</h3><p>全量同步和增量同步</p>
<h4 id="全量同步"><a href="#全量同步" class="headerlink" title="全量同步"></a>全量同步</h4><img src="/2023/02/20/redis/redisMianShi/allSyncData.png" class="" title="img.png">
<ol>
<li>slave发送SYNC命令</li>
<li>master收到并执行 BGSAVE 命令生产 RDB 文件，并使用缓冲区记录此后执行的所有写命令；</li>
<li>master 执行完 BGSAVE 后，向所有的 slave 发送快照文件，并在发送过程中继续记录执行的写命令；</li>
<li>slave 收到快照后，丢弃所有的旧数据，载入收到的数据；</li>
<li>master 快照发送完成后就会开始向 slave 发送缓冲区的写命令；</li>
<li>slave 完成对快照的载入，并开始接受命令请求，执行来自 master 缓冲区的写命令；</li>
<li>slave 完成上面的数据初始化后就可以开始接受用户的读请求了。<br><strong>也可以通过无盘复制来达到目的，由master直接开启一个socket将rdb文件发送给slave服务器</strong></li>
</ol>
<h4 id="部分同步"><a href="#部分同步" class="headerlink" title="部分同步"></a>部分同步</h4><blockquote>
<p>从Redis 2.8开始，如果遭遇连接断开，重新连接之后可以从中断处继续进行复制，而不必重新同步。</p>
<p>它的工作原理是这样：<br>主服务器端为复制流维护一个内存缓冲区（in-memory backlog）。主从服务器都维护一个复制偏移量（replication offset）和master run id ，<br>当连接断开时，从服务器会重新连接上主服务器，然后请求继续复制，假如主从服务器的两个master run id相同，并且指定的偏移量在内存缓冲<br>区中还有效，复制就会从上次中断的点开始继续。如果其中一个条件不满足，就会进行完全重新同步（在2.8版本之前就是直接进行完全重新同步）。<br>因为主运行id不保存在磁盘中，如果从服务器重启了的话就只能进行完全同步了。<br>部分重新同步这个新特性内部使用PSYNC命令，旧的实现中使用SYNC命令。Redis2.8版本可以检测出它所连接的服务器是否支持PSYNC命令，不支持的<br>话使用SYNC命令。</p>
</blockquote>
<h4 id="增量同步"><a href="#增量同步" class="headerlink" title="增量同步"></a>增量同步</h4><ol>
<li>将master接到的写命令也发给slave</li>
</ol>
<h3 id="限制有N个以上从服务器才允许写入"><a href="#限制有N个以上从服务器才允许写入" class="headerlink" title="限制有N个以上从服务器才允许写入"></a>限制有N个以上从服务器才允许写入</h3><blockquote>
<p>从Redis 2.8版本开始，可以配置主服务器连接N个以上从服务器才允许对主服务器进行写操作。但是，因为Redis使用的是异步主从复制，<br>没办法确保从服务器确实收到了要写入的数据，所以还是有一定的数据丢失的可能性。</p>
<p>这一特性的工作原理如下：<br>1）从服务器每秒钟ping一次主服务器，确认处理的复制流数量。<br>2）主服务器记住每个从服务器最近一次ping的时间。<br>3）用户可以配置最少要有N个服务器有小于M秒的确认延迟。<br>4）如果有N个以上从服务器，并且确认延迟小于M秒，主服务器接受写操作。</p>
<p>还可以把这看做是CAP原则（一致性，可用性，分区容错性）不严格的一致性实现，虽然不能百分百确保一致性，但至少保证了丢失的数据不会超过M秒内的数据量。</p>
<p>如果条件不满足，主服务器会拒绝写操作并返回一个错误。<br>1）min-slaves-to-write（最小从服务器数）<br>2）min-slaves-max-lag（从服务器最大确认延迟）</p>
</blockquote>
<h3 id="一致性Hash"><a href="#一致性Hash" class="headerlink" title="一致性Hash"></a>一致性Hash</h3><p>hash环，key 通过 hash 计算之后得到在 hash 环中的位置，然后顺时针方向找到第一个节点，这个节点就是存放 key 的节点。为了解决扩容和宕机问题。<br>因为普通Hash扩容或者宕机时，会影响每个节点上的Key，导致Key大面积失效，而用一致性Hash只会影响相邻的节点<br>同时还可以通过虚拟节点来解决数据倾斜的问题：就是在节点稀疏的 hash 环上对物理节点虚拟出一部分虚拟节点，key 会打到虚拟节点上面，而虚拟节点上的 key 实际也是映射到物理节点上的，这样就避免了数据倾斜导致单节点压力过大导致节点雪崩的问题。</p>
<h3 id="Hash槽"><a href="#Hash槽" class="headerlink" title="Hash槽"></a>Hash槽</h3><p>槽的长度为16384，Redis 集群没有使用一致性hash, 而是引入了哈希槽slots的概念,jedis客户端jar包就是实现了一致性hash算法（客户端模式），或者在redis集群前面加上一层前置代理如Twemproxy也实现了hash一致性算法（代理模式）</p>
<blockquote>
<p>在redis节点发送心跳包时需要把所有的槽放到这个心跳包里，以便让节点知道当前集群信息，16384&#x3D;16k，在发送心跳包时使用char进行bitmap压缩后是2k（2 * 8 (8 bit) * 1024(1k) &#x3D; 16K），也就是说使用2k的空间创建了16k的槽数。 虽然使用CRC16算法最多可以分配65535（2^16-1）个槽位，65535&#x3D;65k，压缩后就是8k（8 * 8 (8 bit) *1024(1k)&#x3D;65K），也就是说需要需要8k的心跳包，作者认为这样做不太值得；并且一般情况下一个redis集群不会有超过1000个master节点，所以16k的槽位是个比较合适的选择。</p>
</blockquote>
<h1 id="大Key优化"><a href="#大Key优化" class="headerlink" title="大Key优化"></a>大Key优化</h1><img src="/2023/02/20/redis/redisMianShi/bigkey.png" class="" title="img.png">

<h1 id="热key优化"><a href="#热key优化" class="headerlink" title="热key优化"></a>热key优化</h1><p>请求到的分片过于集中，超过单台 Server 的性能极限<br><strong>解决办法：</strong><br>1）服务端缓存：即将热点数据缓存至服务端的内存中；<br>2）备份热点Key：即将热点Key+随机数，随机分配至 Redis 其它节点中</p>
<p><strong>如何发现：</strong>   </p>
<ol>
<li>凭借业务经验，进行预估哪些是热key</li>
<li>在客户端进行收集</li>
<li>在Proxy层做收集</li>
<li>用redis自带命令:monitor命令和-hotkeys启动参数</li>
</ol>
]]></content>
      <categories>
        <category>redis</category>
        <category>面试</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>kafka常见面试题以及答案整理</title>
    <url>/2022/08/01/kafka/kafka4/</url>
    <content><![CDATA[<h3 id="Kafka的用途有哪些？使用场景如何？"><a href="#Kafka的用途有哪些？使用场景如何？" class="headerlink" title="Kafka的用途有哪些？使用场景如何？"></a>Kafka的用途有哪些？使用场景如何？</h3><blockquote>
</blockquote>
<h3 id="Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么"><a href="#Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么" class="headerlink" title="Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么"></a>Kafka中的ISR、AR又代表什么？ISR的伸缩又指什么</h3><blockquote>
<p><strong>ISR</strong>：所有与leader副本保持一定程度同步的副本（包括leader副本在内），（In-Sync Replicas）<br><strong>OSR</strong>：与leader副本同步滞后过多或断开连接的副本（不包括leader副本）组成OSR（Out-of-Sync Replicas），<br>时间阈值由replica.lag.time.max.ms配置，默认30S<br><strong>AR</strong>：所有的副本列表统称AR（Assigned Replicas）<br><strong>ISR伸缩</strong>：leader副本负责维护和跟踪 ISR 集合中所有follower副本的滞后状态，当follower副本落后太多或失效时，leader副本会把它从 ISR 集合中剔除。<br>如果 OSR 集合中所有follower副本“追上”了leader副本，那么leader副本会把它从 OSR 集合转移至 ISR 集合。默认情况下，当leader副本发生故障时，只有在 ISR 集合中的follower副本才有资格被选举为新的leader，而在 OSR 集合中的副本则没有任何机会（不过这个可以通过配置来改变</p>
</blockquote>
<h3 id="Kafka中的HW、LEO、LSO、LW等分别代表什么？"><a href="#Kafka中的HW、LEO、LSO、LW等分别代表什么？" class="headerlink" title="Kafka中的HW、LEO、LSO、LW等分别代表什么？"></a>Kafka中的HW、LEO、LSO、LW等分别代表什么？</h3><blockquote>
<p><strong>HW</strong>:High Watermark 高水位线，所有副本中最小的offset,即ISR中副本最小的LEO<br><strong>LEO</strong>:Log End Offset，每个副本当前日志文件中下一条待写入消息的offset，即最新的Offset+1，<br><strong>LSO</strong>:Last Stable Offset,与kafka 事务有关。对于未完成的事务而言，LSO的值等于事务中的第一条消息所在的位置（firstUnstableOffset）；对于已经完成的事务而言，它的值等同于HW相同<br><strong>LW</strong>:Low Watermark,AR集合中最小的LogStartOffset值。<br><strong>Log Start Offset</strong>：每个副本当前日志文件中写入消息的起始offset</p>
</blockquote>
<blockquote>
<p><strong>消费者配置参数：isolation.level</strong>,这个参数用来配置消费者事务的隔离级别。可选值“read_uncommitted”和“read_committed”，表示消费者所消费到<br>的位置，如果设置为“read_committed”，那么消费这就会忽略事务未提交的消息，即只能消费到LSO(LastStableOffset)的位置，<br>默认配置为”read_uncommitted”,即可以消费到HW（High Watermak）的位置。<br><strong>注：follower副本的事务隔离级别也为“read_uncommitted”，并且不可修改。</strong></p>
</blockquote>
<h3 id="Kafka中是怎么体现消息顺序性的？"><a href="#Kafka中是怎么体现消息顺序性的？" class="headerlink" title="Kafka中是怎么体现消息顺序性的？"></a>Kafka中是怎么体现消息顺序性的？</h3><blockquote>
<p><strong>一定条件下，消息单分区内有序</strong>  </p>
<ul>
<li>在kafka  1.x版本之前需要配置<strong>max.in.flight.requests.per.connect&#x3D;1</strong>  </li>
<li>在kafka  1.x版本后，未开启幂等性的情况下必须配置<strong>max.in.flight.requests.per.connect&#x3D;1</strong>，开启幂等性配置（默认开启）可配置<strong>max.in.flight.requests.per.connect&#x3D;5</strong>，<br> 最大为5，因为kafka服务器端会缓存producer5个request的元数据</li>
</ul>
</blockquote>
<h3 id="Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？"><a href="#Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？" class="headerlink" title="Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？"></a>Kafka中的分区器、序列化器、拦截器是否了解？它们之间的处理顺序是什么？</h3><blockquote>
<p>拦截器-&gt;序列化器-&gt;分区器</p>
<ul>
<li><strong>拦截器</strong>：用于Client的定制化逻辑处理，比如说过滤不合规则的数据，补充修改消息内容等等，自定义拦截器可以通过实现ProducerInterceptor（生产者的拦截器）接口  </li>
<li><strong>序列化器</strong>： 序列化数据，防止数据丢失  </li>
<li><strong>分区器</strong>：按照一定规则，将数据划分到不同的分区，若未手动指定分区，则使用默认的分区策略，也可通过实现Partitioner实现自定义分区</li>
</ul>
</blockquote>
<h3 id="Kafka生产者客户端的整体结构是什么样子的？"><a href="#Kafka生产者客户端的整体结构是什么样子的？" class="headerlink" title="Kafka生产者客户端的整体结构是什么样子的？"></a>Kafka生产者客户端的整体结构是什么样子的？</h3><img src="/2022/08/01/kafka/kafka4/producer-design.png" class="" title="img.png">
<h3 id="Kafka生产者客户端中使用了几个线程来处理？分别是什么？"><a href="#Kafka生产者客户端中使用了几个线程来处理？分别是什么？" class="headerlink" title="Kafka生产者客户端中使用了几个线程来处理？分别是什么？"></a>Kafka生产者客户端中使用了几个线程来处理？分别是什么？</h3><blockquote>
<p>俩个，main线程和sender线程,具体作用详见上图</p>
</blockquote>
<h3 id="Kafka的旧版Scala的消费者客户端的设计有什么缺陷？"><a href="#Kafka的旧版Scala的消费者客户端的设计有什么缺陷？" class="headerlink" title="Kafka的旧版Scala的消费者客户端的设计有什么缺陷？"></a>Kafka的旧版Scala的消费者客户端的设计有什么缺陷？</h3><blockquote>
<p>老版本的 Consumer Group 把位移保存在 ZooKeeper 中,这种大吞吐量的写操作会极大地拖慢 ZooKeeper 集群的性能</p>
</blockquote>
<h3 id="“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？"><a href="#“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？" class="headerlink" title="“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？"></a>“消费组中的消费者个数如果超过topic的分区，那么就会有消费者消费不到数据”这句话是否正确？如果不正确，那么有没有什么hack的手段？</h3><blockquote>
<p>一般来说如果消费者过多，出现了消费者的个数大于分区个数的情况，就会有消费者分配不到任何分区,但是可以通过继承AbstractPartitionAssignor<br>实现自定义消费策略，从而实现同一消费组内的任意消费者都可以消费订阅主题的所有分区，其实就是组内广播，</p>
</blockquote>
<h3 id="消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset-1"><a href="#消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset-1" class="headerlink" title="消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?"></a>消费者提交消费位移时提交的是当前消费到的最新消息的offset还是offset+1?</h3><blockquote>
<p>在旧消费者客户端中，消费位移是存储在 ZooKeeper 中的。而在新消费者客户端中，消费位移存储在 Kafka 内部的主题__consumer_offsets 中。<br>当前消费者需要提交的消费位移是offset+1</p>
</blockquote>
<h3 id="有哪些情形会造成重复消费？"><a href="#有哪些情形会造成重复消费？" class="headerlink" title="有哪些情形会造成重复消费？"></a>有哪些情形会造成重复消费？</h3><blockquote>
<ul>
<li><p><strong>Rebalance</strong>:一个consumer正在消费一个分区的一条消息，还没有消费完，发生了rebalance(加入了一个consumer)，从而导致这条消息没有消费成功，rebalance后，另一个consumer又把这条消息消费一遍。</p>
</li>
<li><p><strong>消费者端手动提交</strong>:如果先消费消息，再更新offset位置，导致消息重复消费。</p>
</li>
<li><p><strong>消费者端自动提交</strong>:设置offset为自动提交，关闭kafka时，如果在close之前，调用 consumer.unsubscribe() 则有可能部分offset没提交，下次重启会重复消费。</p>
</li>
<li><p><strong>生产者端</strong>:生产者因为业务问题导致的宕机，在重启之后可能数据会重发</p>
</li>
</ul>
</blockquote>
<h3 id="那些情景下会造成消息漏消费？"><a href="#那些情景下会造成消息漏消费？" class="headerlink" title="那些情景下会造成消息漏消费？"></a>那些情景下会造成消息漏消费？</h3><blockquote>
<ul>
<li><strong>自动提交</strong>:设置offset为自动定时提交，当offset被自动定时提交时，数据还在内存中未处理，此时刚好把线程kill掉，那么offset已经提交，但是数据未处理，导致这部分内存中的数据丢失。</li>
<li><strong>生产者发送消息</strong>:<br>发送消息设置的是fire-and-forget（发后即忘），它只管往 Kafka 中发送消息而并不关心消息是否正确到达。不过在某些时候（比如发生不可重试异常时）会造成消息的丢失。这种发送方式的性能最高，可靠性也最差。</li>
<li><strong>消费者端</strong>:<br>先提交位移，但是消息还没消费完就宕机了，造成了消息没有被消费。自动位移提交同理</li>
<li><strong>acks没有设置为all</strong>:<br>如果在broker还没把消息同步到其他broker的时候宕机了，那么消息将会丢失</li>
</ul>
</blockquote>
<h3 id="KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？"><a href="#KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？" class="headerlink" title="KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？"></a>KafkaConsumer是非线程安全的，那么怎么样实现多线程消费？</h3><img src="/2022/08/01/kafka/kafka4/multi-thread-consumer.png" class="" title="img.png">
<h3 id="简述消费者与消费组之间的关系"><a href="#简述消费者与消费组之间的关系" class="headerlink" title="简述消费者与消费组之间的关系"></a>简述消费者与消费组之间的关系</h3><blockquote>
<p>Consumer Group 下可以有一个或多个 Consumer 实例。这里的实例可以是一个单独的进程，也可以是同一进程下的线程。在实际场景中，使用进程更为常见一些。<br>Consumer Group 下所有实例订阅的主题的单个分区，只能分配给组内的某个 Consumer 实例消费。这个分区当然也可以被其他的 Group 消费</p>
</blockquote>
<h3 id="当你使用kafka-topics-sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？"><a href="#当你使用kafka-topics-sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？" class="headerlink" title="当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？"></a>当你使用kafka-topics.sh创建（删除）了一个topic之后，Kafka背后会执行什么逻辑？</h3><blockquote>
<p>Kafka 会在 log.dir 或 log.dirs 参数所配置的目录下创建相应的主题分区，默认情况下这个目录为&#x2F;tmp&#x2F;kafka-logs&#x2F;。<br>在 ZooKeeper 的&#x2F;brokers&#x2F;topics&#x2F;目录下创建一个同名的实节点，该节点中记录了该主题的分区副本分配方案</p>
</blockquote>
<h3 id="topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？"><a href="#topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？" class="headerlink" title="topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？"></a>topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？</h3><blockquote>
<p>可以增加，使用 kafka-topics 脚本，结合 –alter 参数来增加某个主题的分区数</p>
</blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">kafka-topics.sh --bootstrap-server broker_host:port --alter --topic &lt;topic_name&gt; --partitions &lt;新分区数&gt;</span><br></pre></td></tr></table></figure>
<blockquote>
<p>当分区数增加时，就会触发订阅该主题的所有 Group 开启 Rebalance。<br>首先，Rebalance 过程对 Consumer Group 消费过程有极大的影响。在 Rebalance 过程中，所有 Consumer 实例都会停止消费，等待 Rebalance 完成。这是 Rebalance 为人诟病的一个方面。<br>其次，目前 Rebalance 的设计是所有 Consumer 实例共同参与，全部重新分配所有分区。其实更高效的做法是尽量减少分配方案的变动。<br>最后，Rebalance 实在是太慢了</p>
</blockquote>
<h3 id="topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？"><a href="#topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？" class="headerlink" title="topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？"></a>topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？</h3><blockquote>
<p>不支持，因为删除的分区中的消息不好处理。如果直接存储到现有分区的尾部，消息的时间戳就不会递增，如此对于 Spark、Flink 这类需要消息时间戳（事件时间）的组件将会受到影响；如果分散插入现有的分区，那么在消息量很大的时候，内部的数据复制会占用很大的资源，而且在复制期间，此主题的可用性又如何得到保障？与此同时，顺序性问题、事务性问题，以及分区和副本的状态机切换问题都是不得不面对的</p>
</blockquote>
<h3 id="创建topic时如何选择合适的分区数？"><a href="#创建topic时如何选择合适的分区数？" class="headerlink" title="创建topic时如何选择合适的分区数？"></a>创建topic时如何选择合适的分区数？</h3><blockquote>
<p>可以使用Kafka 本身提供的用于生产者性能测试的 kafka-producer- perf-test.sh 和用于消费者性能测试的 kafka-consumer-perf-test.sh来进行测试。<br>增加合适的分区数可以在一定程度上提升整体吞吐量，但超过对应的阈值之后吞吐量不升反降。如果应用对吞吐量有一定程度上的要求，则建议在投入生产环境之前对同款硬件资源做一个完备的吞吐量相关的测试，以找到合适的分区数阈值区间。<br>分区数的多少还会影响系统的可用性。如果分区数非常多，如果集群中的某个 broker 节点宕机，那么就会有大量的分区需要同时进行 leader 角色切换，这个切换的过程会耗费一笔可观的时间，并且在这个时间窗口内这些分区也会变得不可用。<br>分区数越多也会让 Kafka 的正常启动和关闭的耗时变得越长，与此同时，主题的分区数越多不仅会增加日志清理的耗时，而且在被删除时也会耗费更多的时间。</p>
</blockquote>
<h3 id="Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？"><a href="#Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？" class="headerlink" title="Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？"></a>Kafka目前有那些内部topic，它们都有什么特征？各自的作用又是什么？</h3><blockquote>
<p><strong>__consumer_offsets</strong>：作用是保存 Kafka 消费者的位移信息<br><strong>__transaction_state</strong>：用来存储事务日志消息</p>
</blockquote>
<h3 id="优先副本是什么？它有什么特殊的作用？"><a href="#优先副本是什么？它有什么特殊的作用？" class="headerlink" title="优先副本是什么？它有什么特殊的作用？"></a>优先副本是什么？它有什么特殊的作用？</h3><blockquote>
<p>所谓的优先副本是指在AR集合列表中的第一个副本。理想情况下，优先副本就是该分区的leader 副本，所以也可以称之为 preferred leader。<br>Kafka 要确保所有主题的优先副本在 Kafka 集群中均匀分布，这样就保证了所有分区的 leader 均衡分布。以此来促进集群的负载均衡，这一行为也可以称为“分区平衡”</p>
</blockquote>
<h3 id="Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理"><a href="#Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理" class="headerlink" title="Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理"></a>Kafka有哪几处地方有分区分配的概念？简述大致的过程及原理</h3><blockquote>
<ul>
<li><strong>生产者的分区分配</strong>:是指为每条消息指定其所要发往的分区。可以编写一个具体的类实现org.apache.kafka.clients.producer.Partitioner接口。</li>
<li><strong>消费者中的分区分配</strong>:是指为消费者指定其可以消费消息的分区。Kafka 提供了消费者客户端参数 partition.assignment.strategy 来设置消费者与订阅主题之间的分区分配策略。</li>
<li><strong>分区副本的分配</strong>:是指为集群制定创建主题时的分区副本分配方案，即在哪个 broker 中创建哪些分区的副本。kafka-topics.sh 脚本中提供了一个 replica-assignment 参数来手动指定分区副本的分配方案</li>
</ul>
</blockquote>
<h3 id="简述Kafka的日志目录结构"><a href="#简述Kafka的日志目录结构" class="headerlink" title="简述Kafka的日志目录结构"></a>简述Kafka的日志目录结构</h3><img src="/2022/08/01/kafka/kafka4/log-construct.png" class="" title="img.png">
<h3 id="Kafka中有那些索引文件？"><a href="#Kafka中有那些索引文件？" class="headerlink" title="Kafka中有那些索引文件？"></a>Kafka中有那些索引文件？</h3><blockquote>
<ul>
<li><strong>偏移量索引文件</strong>:用来建立消息偏移量（offset）到物理地址之间的映射关系，方便快速定位消息所在的物理文件位置</li>
<li><strong>时间戳索引文件</strong>:则根据指定的时间戳（timestamp）来查找对应的偏移量信息。</li>
</ul>
</blockquote>
<h3 id="如果我指定了一个offset，Kafka怎么查找到对应的消息？"><a href="#如果我指定了一个offset，Kafka怎么查找到对应的消息？" class="headerlink" title="如果我指定了一个offset，Kafka怎么查找到对应的消息？"></a>如果我指定了一个offset，Kafka怎么查找到对应的消息？</h3><blockquote>
<p>Kafka是通过seek() 方法来指定消费的，在执行seek() 方法之前要去执行一次poll()方法，等到分配到分区之后会去对应的分区的指定位置开始消费，如果指定的位置发生了越界，那么会根据auto.offset.reset 参数设置的情况进行消费。</p>
</blockquote>
<h3 id="如果我指定了一个timestamp，Kafka怎么查找到对应的消息？"><a href="#如果我指定了一个timestamp，Kafka怎么查找到对应的消息？" class="headerlink" title="如果我指定了一个timestamp，Kafka怎么查找到对应的消息？"></a>如果我指定了一个timestamp，Kafka怎么查找到对应的消息？</h3><blockquote>
<p>Kafka提供了一个 offsetsForTimes() 方法，通过 timestamp 来查询与此对应的分区位置。offsetsForTimes() 方法的参数 timestampsToSearch 是一个 Map 类型，key 为待查询的分区，而 value 为待查询的时间戳，该方法会返回时间戳大于等于待查询时间的第一条消息对应的位置和时间戳，对应于 OffsetAndTimestamp 中的 offset 和 timestamp 字段</p>
</blockquote>
<h3 id="聊一聊你对Kafka的Log-Retention的理解"><a href="#聊一聊你对Kafka的Log-Retention的理解" class="headerlink" title="聊一聊你对Kafka的Log Retention的理解"></a>聊一聊你对Kafka的Log Retention的理解</h3><blockquote>
<p><strong>日志删除：</strong> 配置服务端参数log.cleanup.policy:delete(默认就是delete)</p>
<ul>
<li><strong>基于时间：</strong> 日志删除任务会检查当前日志文件中是否有保留时间超过设定的阈值（retentionMs）来寻找可删除的日志分段文件集合（deletableSegments）retentionMs， 可以通过 broker 端参数 log.retention.hours、log.retention.minutes 和 log.retention.ms 来配置，三个配置优先级依次提升。默认情况下只配置了 log.retention.hours 参数，其值为168，即为7天。<br>  删除日志分段时，首先会从 Log 对象中所维护日志分段的跳跃表中移除待删除的日志分段，以保证没有线程对这些日志分段进行读取操作。然后将日志分段所对应的所有文件添加上“.deleted”的后缀（当然也包括对应的索引文件）。最后交由一个以“delete-file”命名的延迟任务来删除这些以“.deleted”为后缀的文件，这个任务的延迟执行时间可以通过 file.delete.delay.ms 参数来调配，此参数的默认值为60000，即1分钟。</li>
<li><strong>基于大小：</strong> 日志删除任务会检查当前日志的大小是否超过设定的阈值（retentionSize）来寻找可删除的日志分段的文件集合（deletableSegments）。<br>  retentionSize 可以通过 broker 端参数 log.retention.bytes 来配置，默认值为-1，表示无穷大。注意 log.retention.bytes 配置的是 Log 中所有日志文件的总大小，而不是单个日志分段（确切地说应该为 .log 日志文件）的大小。单个日志分段的大小由 broker 端参数 log.segment.bytes 来限制，默认值为1073741824，即 1GB</li>
<li><strong>基于日志偏移量：</strong> 这个无法配置，一般不关注，一般情况下日志文件的起始偏移量logStartOffset（logStartOffset值是整个 Log 对象对外可见消息的最小位移值）等于第一个日志分段的baseOffset，但是这并不是绝对的，logStartOffset的值可以通过DeleteRecordsRequest请求、日志的清理和截断等操作修改。</li>
</ul>
</blockquote>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">log.retention.hours=168 //7d</span><br><span class="line">log.retention.check.interval.ms=300000 //5min log过期检查时间间隔</span><br><span class="line">log.segment.bytes=1073741824 //1G</span><br><span class="line">log.cleaner.delete.retention.ms=86400000 // 1d 标记为deleted的segment的保留时间</span><br><span class="line">log.cleaner.backoff.ms=15000 //15s 清理线程扫描间隔</span><br></pre></td></tr></table></figure>
<h3 id="聊一聊你对Kafka的Log-Compaction的理解"><a href="#聊一聊你对Kafka的Log-Compaction的理解" class="headerlink" title="聊一聊你对Kafka的Log Compaction的理解"></a>聊一聊你对Kafka的Log Compaction的理解</h3><blockquote>
<p><strong>日志压缩：</strong> 配置服务端参数log.cleanup.policy:compact<br>Log Compaction 对于有相同 key 的不同 value 值，只保留最后一个版本。如果应用只关心 key 对应的最新 value 值，则可以开启 Kafka 的日志清理功能，Kafka 会定期将相同 key 的消息进行合并，只保留最新的 value 值，一般可用于用户信息存储等</p>
</blockquote>
<h3 id="聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）"><a href="#聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）" class="headerlink" title="聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）"></a>聊一聊你对Kafka底层存储的理解（页缓存、内核层、块层、设备层）</h3><blockquote>
<p><strong>页缓存：</strong> 页缓存是操作系统实现的一种主要的磁盘缓存，以此用来减少对磁盘 I&#x2F;O 的操作。具体来说，就是把磁盘中的数据缓存到内存中，把对磁盘的访问变为对内存的访问，基于这些因素，使用文件系统并依赖于页缓存的做法明显要优于维护一个进程内缓存或其他结构，至少我们可以省去了一份进程内部的缓存消耗，同时还可以通过结构紧凑的字节码来替代使用对象的方式以节省更多的空间。<br>此外，即使 Kafka 服务重启，页缓存还是会保持有效，然而进程内的缓存却需要重建。这样也极大地简化了代码逻辑，因为维护页缓存和文件之间的一致性交由操作系统来负责，这样会比进程内维护更加安全有效。  </p>
<p><strong>零拷贝：</strong> 除了消息顺序追加、页缓存等技术，Kafka 还使用零拷贝（Zero-Copy）技术来进一步提升性能。所谓的零拷贝是指将数据直接从磁盘文件复制到网卡设备中，而不需要经由应用程序之手。零拷贝大大提高了应用程序的性能，减少了内核和用户模式之间的上下文切换。对 Linux 操作系统而言，零拷贝技术依赖于底层的 sendfile() 方法实现。对应于 Java 语言，FileChannal.transferTo() 方法的底层实现就是 sendfile() 方法   </p>
</blockquote>
<blockquote>
<p>下图中左侧为传统方式，右侧为零拷贝，详细可见：<a href="https://developer.ibm.com/articles/j-zerocopy/">https://developer.ibm.com/articles/j-zerocopy/</a></p>
</blockquote>
<img src="/2022/08/01/kafka/kafka4/zero-copy.png" class="" title="img.png">
<h3 id="聊一聊Kafka的延时操作的原理"><a href="#聊一聊Kafka的延时操作的原理" class="headerlink" title="聊一聊Kafka的延时操作的原理"></a>聊一聊Kafka的延时操作的原理</h3><blockquote>
<p>Kafka 中有多种延时操作，比如延时生产，还有延时拉取（DelayedFetch）、延时数据删除（DelayedDeleteRecords）等。<br>延时操作创建之后会被加入延时操作管理器（DelayedOperationPurgatory）来做专门的处理。延时操作有可能会超时，每个延时操作管理器都会配备一个定时器（SystemTimer）来做超时管理，定时器的底层就是采用时间轮（TimingWheel）实现的。</p>
</blockquote>
<h3 id="聊一聊Kafka控制器的作用"><a href="#聊一聊Kafka控制器的作用" class="headerlink" title="聊一聊Kafka控制器的作用"></a>聊一聊Kafka控制器的作用</h3><blockquote>
<p><strong>controller选举：</strong> kafka集群在创建时，会在Zookeeper中创建临时节点 <KafkaZkChroot>&#x2F;controller,创建成功的那个broker为此次的controller,<br>其他的broker会对该控制器节点创建watch对象，监听该节点的变更，当控制器失效后，其他broker会进行抢注，当选新的controller  </p>
<p><strong>controller作用：</strong>  </p>
<ul>
<li>_主题管理_： 主题的创建、删除、修改分区；kafka-topics 脚本相关后台操作基本上都是由controller帮我们完成的</li>
<li>_分区重新分配_：kafka-reassign-partitions 脚本提供的对已有主题分区进行细粒度的分配功能</li>
<li>_Preferred 领导者选举_：当某个broker节点下线重新上线后，该broker节点上的所有分区副本均为Follower，会使分区Leader分布不均匀，这个可以协调Leader</li>
<li>_broker管理_： 自动检测broker的新增、宕机，依赖于利用Watch 机制检查 ZooKeeper 的 &#x2F;brokers&#x2F;ids 节点下的子节点数量变更，因为每个节点在启动后都会在<br>此节点下创建临时节点；</li>
<li>_存储集群数据_：向其他 Broker 提供数据服务。控制器上保存了最全的集群元数据信息，其他所有 Broker 会定期接收控制器发来的元数据更新请求，从而更新其内存中的缓存数据</li>
</ul>
</blockquote>
<h3 id="消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）"><a href="#消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）" class="headerlink" title="消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）"></a>消费再均衡的原理是什么？（提示：消费者协调器和消费组协调器）</h3><ul>
<li><strong><em>名词解释</em></strong><blockquote>
<p><strong>消费者协调器（ConsumerCoordinator）：</strong> 位于消费者客户端的组件，负责与GroupCoordinator通信<br><strong>消费组协调器（GroupCoordinator:）</strong> 位于kafka服务端，用于管理消费组的组件</p>
</blockquote>
</li>
<li><strong><em>什么时候会发生消费者再均衡？</em></strong><blockquote>
<ol>
<li>有新的消费者加入消费组或者宕机下线（不一定真的宕机，有可能只是长时间未与GroupCoordinator发送心跳，默认45秒就被判定掉线）</li>
<li>消费者主动退出消费组（发送 LeaveGroupRequest 请求）。比如客户端调用了 unsubscrible() 方法取消对某些主题的订阅</li>
<li>消费组所对应的 GroupCoorinator 节点发生了变更</li>
<li>消费组内所订阅的任一主题或者主题的分区数量发生变化。</li>
</ol>
</blockquote>
</li>
<li><strong><em>消费者消费过程</em></strong><blockquote>
<ol>
<li>FIND_COORDINATOR:确定消费者组对应的GroupCoordinator所在broker，并创建相互通信的网络连接，若连接不正常，就需要向集群中的负载最小的节点发送 FindCoordinatorRequest 请求来查找对应的 GroupCoordinator。  </li>
<li>JOIN_GROUP:消费者会向 GroupCoordinator 发送 JoinGroupRequest 请求，并处理响应。且会选择出一个消费者的Leader，并选出大多消费者支持的分区副本分配策略</li>
<li>SYNC_GROUP：消费者Leader将根据阶段二选出分配策略，实施具体的分配方案，并将方案通过GroupCoordinator同步给各个消费者</li>
<li>HEARTBEAT：消费者通过向GroupCoordinator发送心跳来维护自己的活跃性，默认每3秒一次，45秒超时，且心跳和消费是俩个独立的线程</li>
</ol>
</blockquote>
</li>
</ul>
<h3 id="Kafka中的幂等是怎么实现的"><a href="#Kafka中的幂等是怎么实现的" class="headerlink" title="Kafka中的幂等是怎么实现的"></a>Kafka中的幂等是怎么实现的</h3><blockquote>
<p>Kafka为此引入了producerId（简称 PID）和序列号（sequence number）这两个概念,每个生产者实例在初始化的时候会被分配一个PID（用户无感知，可以日志文件里查看），<br>对于每个PID，消息发送到的每一个分区都有对应的序列号，这些序列号从0开始单调递增。生产者每发送一条消息就会将 &lt;PID，分区&gt; 对应的序列号的值加1，为每一对 &lt;PID，分区&gt; 维护一个序列号，<br>当出现乱序时，生产者会抛出 OutOfOrderSequenceException</p>
</blockquote>
<h3 id="Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话…只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ…-”）"><a href="#Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话…只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ…-”）" class="headerlink" title="Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话…只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ….”）"></a>Kafka中的事务是怎么实现的（这题我去面试6家被问4次，照着答案念也要念十几分钟，面试官简直凑不要脸。实在记不住的话…只要简历上不写精通Kafka一般不会问到，我简历上写的是“熟悉Kafka，了解RabbitMQ….”）</h3><blockquote>
<p>kafka事务是以幂等性为前提，生产者配置一个transactionalId而生效的；每个transactionalId和PID相对应<br>每次发送数据给&lt;Topic, Partition&gt;前，需要先向事务协调器发送AddPartitionsToTxnRequest，事务协调器会将该&lt;Transaction, Topic, Partition&gt;存于__transaction_state内，并将其状态置为BEGIN。</p>
<p>在处理完 AddOffsetsToTxnRequest 之后，生产者还会发送 TxnOffsetCommitRequest 请求给 GroupCoordinator，从而将本次事务中包含的消费位移信息 offsets 存储到主题 __consumer_offsets 中</p>
<p>一旦上述数据写入操作完成，应用程序必须调用KafkaProducer的commitTransaction方法或者abortTransaction方法以结束当前事务。无论调用 commitTransaction() 方法还是 abortTransaction() 方法，生产者都会向 TransactionCoordinator 发送 EndTxnRequest 请求。<br>TransactionCoordinator 在收到 EndTxnRequest 请求后会执行如下操作：</p>
<p>将 PREPARE_COMMIT 或 PREPARE_ABORT 消息写入主题 __transaction_state<br>通过 WriteTxnMarkersRequest 请求将 COMMIT 或 ABORT 信息写入用户所使用的普通主题和 __consumer_offsets<br>将 COMPLETE_COMMIT 或 COMPLETE_ABORT 信息写入内部主题 __transaction_state标明该事务结束<br>在消费端有一个参数isolation.level，设置为“read_committed”，表示消费端应用不可以看到尚未提交的事务内的消息。如果生产者开启事务并向某个分区值发送3条消息 msg1、msg2 和 msg3，在执行 commitTransaction() 或 abortTransaction() 方法前，设置为“read_committed”的消费端应用是消费不到这些消息的，不过在 KafkaConsumer 内部会缓存这些消息，直到生产者执行 commitTransaction() 方法之后它才能将这些消息推送给消费端应用。反之，如果生产者执行了 abortTransaction() 方法，那么 KafkaConsumer 会将这些缓存的消息丢弃而不推送给消费端应用。</p>
</blockquote>
<h3 id="Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？"><a href="#Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？" class="headerlink" title="Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？"></a>Kafka中有那些地方需要选举？这些地方的选举策略又有哪些？</h3><blockquote>
</blockquote>
<h3 id="失效副本是指什么？有那些应对措施？"><a href="#失效副本是指什么？有那些应对措施？" class="headerlink" title="失效副本是指什么？有那些应对措施？"></a>失效副本是指什么？有那些应对措施？</h3><blockquote>
<p>正常情况下，分区的所有副本都处于 ISR 集合中，但是难免会有异常情况发生，从而某些副本被剥离出 ISR 集合中。在 ISR 集合之外，也就是处于同步失效或功能失效（比如副本处于非存活状态）的副本统称为失效副本，失效副本对应的分区也就称为同步失效分区，即 under-replicated 分区</p>
<p>一般有这几种情况会导致副本失效：</p>
<ul>
<li>follower 副本进程卡住，在一段时间内根本没有向 leader 副本发起同步请求，比如频繁的 Full GC。</li>
<li>follower 副本进程同步过慢，在一段时间内都无法追赶上 leader 副本，比如 I&#x2F;O 开销过大。</li>
<li>如果通过工具增加了副本因子，那么新增加的副本在赶上 leader 副本之前也都是处于失效状态的。</li>
<li>如果一个 follower 副本由于某些原因（比如宕机）而下线，之后又上线，在追赶上 leader 副本之前也处于失效状态</li>
</ul>
</blockquote>
<h3 id="多副本下，各个副本中的HW和LEO的演变过程"><a href="#多副本下，各个副本中的HW和LEO的演变过程" class="headerlink" title="多副本下，各个副本中的HW和LEO的演变过程"></a>多副本下，各个副本中的HW和LEO的演变过程</h3><h3 id="为什么Kafka不支持读写分离？"><a href="#为什么Kafka不支持读写分离？" class="headerlink" title="为什么Kafka不支持读写分离？"></a>为什么Kafka不支持读写分离？</h3><blockquote>
<p><strong>数据一致性问题:</strong> 数据从主节点转到从节点必然会有一个延时的时间窗口，这个时间窗口会导致主从节点之间的数据不一致。<br><strong>延时问题:</strong> 数据从写入主节点到同步至从节点中的过程需要经历网络→主节点内存→主节点磁盘→网络→从节点内存→从节点磁盘这几个阶段。对延时敏感的应用而言，主写从读的功能并不太适用。  </p>
</blockquote>
<p>对于Kafka来说，必要性不是很高，因为在Kafka集群中，如果存在多个副本，经过合理的配置，可以让leader副本均匀的分布在各个broker上面，使每个 broker 上的读写负载都是一样的。</p>
<h3 id="Kafka在可靠性方面做了哪些改进？（HW-LeaderEpoch）"><a href="#Kafka在可靠性方面做了哪些改进？（HW-LeaderEpoch）" class="headerlink" title="Kafka在可靠性方面做了哪些改进？（HW, LeaderEpoch）"></a>Kafka在可靠性方面做了哪些改进？（HW, LeaderEpoch）</h3><blockquote>
<p><strong>HW</strong>: HW 是 High Watermark 的缩写，俗称高水位，它标识了一个特定的消息偏移量（offset），消费者只能拉取到这个 offset 之前的消息。<br>分区 ISR 集合中的每个副本都会维护自身的 LEO，而 ISR 集合中最小的 LEO 即为分区的 HW，对消费者而言只能消费 HW 之前的消息</p>
<p><strong>LeaderEpoch:</strong> 代表 leader 的纪元信息（epoch），初始值为0。每当 leader 变更一次，leader epoch 的值就会加1<br>leader epoch 的值就会加1，相当于为 leader 增设了一个版本号。每个副本中还会增设一个矢量 &lt;LeaderEpoch &#x3D;&gt; StartOffset&gt;，<br>其中 StartOffset 表示当前 LeaderEpoch 下写入的第一条消息的偏移量。</p>
</blockquote>
<p>假设有两个节点A和B，B是leader节点，里面的数据如图：</p>
<img src="/2022/08/01/kafka/kafka4/leaderEpoch1.png" class="" title="img.png">  
<p>A发生重启，之后A不是先忙着截断日志而是先发送OffsetsForLeaderEpochRequest请求给B，B作为目前的leader在收到请求之后会返回当前的LEO（LogEndOffset，注意图中LE0和LEO的不同），与请求对应的响应为OffsetsForLeaderEpochResponse。如果 A 中的 LeaderEpoch（假设为 LE_A）和 B 中的不相同，那么 B 此时会查找 LeaderEpoch 为 LE_A+1 对应的 StartOffset 并返回给 A</p>
<img src="/2022/08/01/kafka/kafka4/leaderEpoch2.png" class="" title="img_1.png">  
<p>如上图所示，A 在收到2之后发现和目前的 LEO 相同，也就不需要截断日志了，以此来保护数据的完整性。</p>
<p>再如，之后 B 发生了宕机，A 成为新的 leader，那么对应的 LE&#x3D;0 也变成了 LE&#x3D;1，对应的消息 m2 此时就得到了保留。后续的消息都可以以 LE1 为 LeaderEpoch 陆续追加到 A 中。这个时候A就会有两个LE，第二LE所记录的Offset从2开始。如果B恢复了，那么就会从A中获取到LE+1的Offset为2的值返回给B。</p>
<img src="/2022/08/01/kafka/kafka4/leaderEpoch3.png" class="" title="img_2.png">  
<p>再来看看LE如何解决数据不一致的问题：<br>当前 A 为 leader，B 为 follower，A 中有2条消息 m1 和 m2，而 B 中有1条消息 m1。假设 A 和 B 同时“挂掉”，然后 B 第一个恢复过来并成为新的 leader。</p>
<img src="/2022/08/01/kafka/kafka4/leaderEpoch4.png" class="" title="img_3.png">  
<p>之后 B 写入消息 m3，并将 LEO 和 HW 更新至2，如下图所示。注意此时的 LeaderEpoch 已经从 LE0 增至 LE1 了。</p>
<img src="/2022/08/01/kafka/kafka4/leaderEpoch5.png" class="" title="img_4.png">  
<p>紧接着 A 也恢复过来成为 follower 并向 B 发送 OffsetsForLeaderEpochRequest 请求，此时 A 的 LeaderEpoch 为 LE0。B 根据 LE0 查询到对应的 offset 为1并返回给 A，A 就截断日志并删除了消息 m2，如下图所示。之后 A 发送 FetchRequest 至 B 请求来同步数据，最终A和B中都有两条消息 m1 和 m3，HW 和 LEO都为2，并且 LeaderEpoch 都为 LE1，如此便解决了数据不一致的问题。</p>
<img src="/2022/08/01/kafka/kafka4/leaderEpoch6.png" class="" title="img_5.png">  

<h3 id="Kafka中怎么实现死信队列和重试队列？"><a href="#Kafka中怎么实现死信队列和重试队列？" class="headerlink" title="Kafka中怎么实现死信队列和重试队列？"></a>Kafka中怎么实现死信队列和重试队列？</h3><h3 id="Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）"><a href="#Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）" class="headerlink" title="Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）"></a>Kafka中的延迟队列怎么实现（这题被问的比事务那题还要多！！！听说你会Kafka，那你说说延迟队列怎么实现？）</h3><h3 id="Kafka中怎么做消息审计？"><a href="#Kafka中怎么做消息审计？" class="headerlink" title="Kafka中怎么做消息审计？"></a>Kafka中怎么做消息审计？</h3><blockquote>
<p>消息审计是指在消息生产、存储和消费的整个过程之间对消息个数及延迟的审计，以此来检测是否有数据丢失、是否有数据重复、端到端的延迟又是多少等内容。</p>
<p>目前与消息审计有关的产品也有多个，比如 Chaperone（Uber）、Confluent Control Center、Kafka Monitor（LinkedIn），它们主要通过在消息体（value 字段）或在消息头（headers 字段）中内嵌消息对应的时间戳 timestamp 或全局的唯一标识 ID（或者是两者兼备）来实现消息的审计功能。</p>
<p>内嵌 timestamp 的方式主要是设置一个审计的时间间隔 time_bucket_interval（可以自定义设置几秒或几分钟），根据这个 time_bucket_interval 和消息所属的 timestamp 来计算相应的时间桶（time_bucket）。</p>
<p>内嵌 ID 的方式就更加容易理解了，对于每一条消息都会被分配一个全局唯一标识 ID。如果主题和相应的分区固定，则可以为每个分区设置一个全局的 ID。当有消息发送时，首先获取对应的 ID，然后内嵌到消息中，最后才将它发送到 broker 中。消费者进行消费审计时，可以判断出哪条消息丢失、哪条消息重复。</p>
</blockquote>
<h3 id="Kafka中怎么做消息轨迹？"><a href="#Kafka中怎么做消息轨迹？" class="headerlink" title="Kafka中怎么做消息轨迹？"></a>Kafka中怎么做消息轨迹？</h3><blockquote>
<p>消息轨迹指的是一条消息从生产者发出，经由 broker 存储，再到消费者消费的整个过程中，各个相关节点的状态、时间、地点等数据汇聚而成的完整链路信息。生产者、broker、消费者这3个角色在处理消息的过程中都会在链路中增加相应的信息，将这些信息汇聚、处理之后就可以查询任意消息的状态，进而为生产环境中的故障排除提供强有力的数据支持。</p>
<p>对消息轨迹而言，最常见的实现方式是封装客户端，在保证正常生产消费的同时添加相应的轨迹信息埋点逻辑。无论生产，还是消费，在执行之后都会有相应的轨迹信息，我们需要将这些信息保存起来。</p>
<p>我们同样可以将轨迹信息保存到 Kafka 的某个主题中，比如下图中的主题 trace_topic。</p>
</blockquote>
<img src="/2022/08/01/kafka/kafka4/messageTrace.png" class="" title="img.png">
<h3 id="Kafka中有那些配置参数比较有意思？聊一聊你的看法"><a href="#Kafka中有那些配置参数比较有意思？聊一聊你的看法" class="headerlink" title="Kafka中有那些配置参数比较有意思？聊一聊你的看法"></a>Kafka中有那些配置参数比较有意思？聊一聊你的看法</h3><h3 id="Kafka中有那些命名比较有意思？聊一聊你的看法"><a href="#Kafka中有那些命名比较有意思？聊一聊你的看法" class="headerlink" title="Kafka中有那些命名比较有意思？聊一聊你的看法"></a>Kafka中有那些命名比较有意思？聊一聊你的看法</h3><h3 id="Kafka有哪些指标需要着重关注？"><a href="#Kafka有哪些指标需要着重关注？" class="headerlink" title="Kafka有哪些指标需要着重关注？"></a>Kafka有哪些指标需要着重关注？</h3><blockquote>
<p>比较重要的 Broker 端 JMX 指标：</p>
<ul>
<li>BytesIn&#x2F;BytesOut：即 Broker 端每秒入站和出站字节数。你要确保这组值不要接近你的网络带宽，否则这通常都表示网卡已被“打满”，很容易出现网络丢包的情形。</li>
<li>NetworkProcessorAvgIdlePercent：即网络线程池线程平均的空闲比例。通常来说，你应该确保这个 JMX 值长期大于 30%。如果小于这个值，就表明你的网络线程池非常繁忙，你需要通过增加网络线程数或将负载转移给其他服务器的方式，来给该 Broker 减负。</li>
<li>RequestHandlerAvgIdlePercent：即 I&#x2F;O 线程池线程平均的空闲比例。同样地，如果该值长期小于 30%，你需要调整 I&#x2F;O 线程池的数量，或者减少 Broker 端的负载。</li>
<li>UnderReplicatedPartitions：即未充分备份的分区数。所谓未充分备份，是指并非所有的 Follower 副本都和 Leader 副本保持同步。一旦出现了这种情况，通常都表明该分区有可能会出现数据丢失。因此，这是一个非常重要的 JMX 指标。</li>
<li>ISRShrink&#x2F;ISRExpand：即 ISR 收缩和扩容的频次指标。如果你的环境中出现 ISR 中副本频繁进出的情形，那么这组值一定是很高的。这时，你要诊断下副本频繁进出 ISR 的原因，并采取适当的措施。</li>
<li>ActiveControllerCount：即当前处于激活状态的控制器的数量。正常情况下，Controller 所在 Broker 上的这个 JMX 指标值应该是 1，其他 Broker 上的这个值是 0。如果你发现存在多台 Broker 上该值都是 1 的情况，一定要赶快处理，处理方式主要是查看网络连通性。这种情况通常表明集群出现了脑裂。脑裂问题是非常严重的分布式故障，Kafka 目前依托 ZooKeeper 来防止脑裂。但一旦出现脑裂，Kafka 是无法保证正常工作的。</li>
</ul>
</blockquote>
<h3 id="怎么计算Lag？-注意read-uncommitted和read-committed状态下的不同"><a href="#怎么计算Lag？-注意read-uncommitted和read-committed状态下的不同" class="headerlink" title="怎么计算Lag？(注意read_uncommitted和read_committed状态下的不同)"></a>怎么计算Lag？(注意read_uncommitted和read_committed状态下的不同)</h3><blockquote>
<p>如果消费者客户端的 isolation.level 参数配置为“read_uncommitted”（默认）,它对应的 Lag 等于HW – ConsumerOffset 的值，其中 ConsumerOffset 表示当前的消费位移。</p>
<p>如果这个参数配置为“read_committed”，那么就要引入 LSO 来进行计算了。LSO 是 LastStableOffset 的缩写,它对应的 Lag 等于 LSO – ConsumerOffset 的值。</p>
<ul>
<li>首先通过 DescribeGroupsRequest 请求获取当前消费组的元数据信息，当然在这之前还会通过 FindCoordinatorRequest 请求查找消费组对应的 GroupCoordinator。</li>
<li>接着通过 OffsetFetchRequest 请求获取消费位移 ConsumerOffset。</li>
<li>然后通过 KafkaConsumer 的 endOffsets(Collection partitions)方法（对应于 ListOffsetRequest 请求）获取 HW（LSO）的值。</li>
<li>最后通过 HW 与 ConsumerOffset 相减得到分区的 Lag，要获得主题的总体 Lag 只需对旗下的各个分区累加即可。</li>
</ul>
</blockquote>
<h3 id="Kafka的那些设计让它有如此高的性能？"><a href="#Kafka的那些设计让它有如此高的性能？" class="headerlink" title="Kafka的那些设计让它有如此高的性能？"></a>Kafka的那些设计让它有如此高的性能？</h3><blockquote>
<ul>
<li><strong>分区：</strong> 主题topic会有多个分区，kafka将分区均匀地分配到整个集群中，当生产者向对应主题传递消息，消息通过负载均衡机制传递到不同的分区以减轻单个服务器实例的压力。<br>一个Consumer Group中可以有多个consumer，多个consumer可以同时消费不同分区的消息，大大的提高了消费者的并行消费能力</li>
<li><strong>网络传输：</strong> 采用批量发送和拉取和端到端的信息压缩，（kafaka会将这些批量的数据进行压缩，将一批消息打包后进行压缩，发送broker服务器后，最终这些数据还是提供给消费者用，所以数据在服务器上还是保持压缩状态，不会进行解压，而且频繁的压缩和解压也会降低性能，最终还是以压缩的方式传递到消费者的手上）</li>
<li><strong>顺序读写：</strong> kafka将消息追加到日志文件中，利用了磁盘的顺序读写，来提高读写效率    </li>
<li><strong>零拷贝：</strong> 零拷贝将文件内容从磁盘通过DMA引擎复制到内核缓冲区，而且没有把数据复制到socket缓冲区，只是将数据位置和长度信息的描述符复制到了socket缓存区，然后直接将数据传输到网络接口，最后发送。这样大大减小了拷贝的次数，提高了效率。kafka正是调用linux系统给出的sendfile系统调用来使用零拷贝。Java中的系统调用给出的是FileChannel.transferTo接口。</li>
<li><strong>存储机制：</strong> 如果分区规则设置得合理，那么所有的消息可以均匀地分布到不同的分区中，这样就可以实现水平扩展。不考虑多副本的情况，一个分区对应一个日志（Log）。为了防止 Log 过大，Kafka 又引入了日志分段（LogSegment）的概念，将 Log 切分为多个 LogSegment，相当于一个巨型文件被平均分配为多个相对较小的文件，这样也便于消息的维护和清理。  <img src="/2022/08/01/kafka/kafka4/log-construct.png" class="" title="img.png">
Kafka 中的索引文件以稀疏索引（sparse index）的方式构造消息的索引，它并不保证每个消息在索引文件中都有对应的索引项。每当写入一定量（由 broker 端参数 log.index.interval.bytes 指定，默认值为4096，即 4KB）的消息时，偏移量索引文件和时间戳索引文件分别增加一个偏移量索引项和时间戳索引项，增大或减小 log.index.interval.bytes 的值，对应地可以增加或缩小索引项的密度。</li>
</ul>
</blockquote>
<h3 id="Kafka有什么优缺点？"><a href="#Kafka有什么优缺点？" class="headerlink" title="Kafka有什么优缺点？"></a>Kafka有什么优缺点？</h3><h3 id="还用过什么同质类的其它产品，与Kafka相比有什么优缺点？"><a href="#还用过什么同质类的其它产品，与Kafka相比有什么优缺点？" class="headerlink" title="还用过什么同质类的其它产品，与Kafka相比有什么优缺点？"></a>还用过什么同质类的其它产品，与Kafka相比有什么优缺点？</h3><h3 id="为什么选择Kafka"><a href="#为什么选择Kafka" class="headerlink" title="为什么选择Kafka?"></a>为什么选择Kafka?</h3><h3 id="在使用Kafka的过程中遇到过什么困难？怎么解决的？"><a href="#在使用Kafka的过程中遇到过什么困难？怎么解决的？" class="headerlink" title="在使用Kafka的过程中遇到过什么困难？怎么解决的？"></a>在使用Kafka的过程中遇到过什么困难？怎么解决的？</h3><h3 id="怎么样才能确保Kafka极大程度上的可靠性？"><a href="#怎么样才能确保Kafka极大程度上的可靠性？" class="headerlink" title="怎么样才能确保Kafka极大程度上的可靠性？"></a>怎么样才能确保Kafka极大程度上的可靠性？</h3><h3 id="聊一聊你对Kafka生态的理解"><a href="#聊一聊你对Kafka生态的理解" class="headerlink" title="聊一聊你对Kafka生态的理解"></a>聊一聊你对Kafka生态的理解</h3>]]></content>
      <tags>
        <tag>kafka 面试题</tag>
      </tags>
  </entry>
  <entry>
    <title>mysql索引简介</title>
    <url>/2022/10/13/mysql/mysql-index/</url>
    <content><![CDATA[<h1 id="数据页"><a href="#数据页" class="headerlink" title="数据页"></a>数据页</h1><h2 id="InnoDB-是如何存储数据的？"><a href="#InnoDB-是如何存储数据的？" class="headerlink" title="InnoDB 是如何存储数据的？"></a>InnoDB 是如何存储数据的？</h2><p>记录是按照行来存储的，但是数据库的读取并不是以行为单位，否则一次读取(一次 I&#x2F;O 操作) 只能处理一行数据，效率会非常低。因此，InnoDB 的数据是按 数据页<br>数据页为单位来读写的，也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。</p>
<p>数据库的 I&#x2F;O 操作的最小单位是页，InnoDB 数据页的默认大小是 16KB，数据库每次读写都是以 16KB 为单位，一次最少从磁盘中读取 16K 的内容到内存中 或者 一次最少把 16K 内容刷新到磁盘中。</p>
<p>数据页包括七个部分，结构如下：</p>
<img src="/2022/10/13/mysql/mysql-index/data-page.png" class="" title="img.png">

<p>7个部分的说明如下：</p>
<table>
<thead>
<tr>
<th>名称</th>
<th>说明</th>
</tr>
</thead>
<tbody><tr>
<td>File Header</td>
<td>文件头，表示页的信息</td>
</tr>
<tr>
<td>Page Header</td>
<td>页头，表示页的状态信息</td>
</tr>
<tr>
<td>Infimum + Supremum Records</td>
<td>两个虚拟伪记录，分别表示页中的最小记录和最大记录</td>
</tr>
<tr>
<td>User Records</td>
<td>用户记录，存储行记录内容</td>
</tr>
<tr>
<td>Free Space</td>
<td>空闲空间，表示页中还未被使用的空间</td>
</tr>
<tr>
<td>Page Directory</td>
<td>页目录，存储用户记录的相对位置，对记录起到索引作用</td>
</tr>
<tr>
<td>Fil Trailer</td>
<td>文件尾，校验页是否完整</td>
</tr>
</tbody></table>
<p>在 File Header 中有两个指针(FIL_PAGE_PREV, FIL_PAGE_NEXT)，分别指向上一个数据页和下一个数据页，连接起来的页相当于一个双向的链表，如下图：</p>
<h2 id="数据页中的-User-Records-是如何组织的"><a href="#数据页中的-User-Records-是如何组织的" class="headerlink" title="数据页中的 User Records 是如何组织的"></a>数据页中的 User Records 是如何组织的</h2><p>User Reocrds 是用来存储用户数据的，各条记录之间通过 next_record 字段串联成了一个链表。除了用户的行记录之外，还有两条记录: Infimum(最小行) 和 Supermum(最大行)。这是 InnoDB<br>在创建表时自动生成的。如下图：</p>
<img src="/2022/10/13/mysql/mysql-index/data-pages.png" class="" title="img.png">

<h2 id="页目录-Page-Directory"><a href="#页目录-Page-Directory" class="headerlink" title="页目录(Page Directory)"></a>页目录(Page Directory)</h2><p>数据页中的记录按照索引键值顺序组成单项链表，单向链表的特点是插入、删除很高效，但是检索效率不高，最差的情况下需要遍历链表上的所有节点才能完成检索。 因此，数据页中有一个页目录(Page Directory)，起到记录的 “索引” 作用。</p>
<h3 id="页目录创建过程"><a href="#页目录创建过程" class="headerlink" title="页目录创建过程"></a>页目录创建过程</h3><ul>
<li>将所有的记录划分成几个组，这些记录包括 Infimum 和 Supermum</li>
<li>每个记录组的最后一条记录就是组内最大的那条记录，并且最后一条记录的头信息中会存储该组一共有多少条记录，作为 n_owned 字</li>
<li>页目录用来存储每组最后一条记录的地址偏移量，这些地址偏移量会按照先后顺序存储起来，每组的地址偏移量也被称之为槽（slots），每个槽相当于指针指向了不同组的最后一个记录。</li>
</ul>
<h3 id="槽中可以放多少数据行"><a href="#槽中可以放多少数据行" class="headerlink" title="槽中可以放多少数据行"></a>槽中可以放多少数据行</h3><ul>
<li>Infimum 只能包含一条记录</li>
<li>Supermum 可以是 [1,8] 条记录</li>
<li>其他的则是 [4,8] 条记录</li>
</ul>
<h3 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h3><img src="/2022/10/13/mysql/mysql-index/data-page-demo.png" class="" title="img.png">

<p>页目录就是多个槽组成的，槽相当于分组记录的索引。因为记录是按照索引键值顺序存放，所以我们通过槽查找记录时，可以使用二分法快速定位要查询的记录在哪个槽(哪个记录分组)，定位到槽后，再遍历槽内的所有记录，无需从最小记录开始遍历整个页中的记录。</p>
<p>以上图为例，5 个槽的编号分别为 0，1，2，3，4，我想查找主键为 10 的用户记录</p>
<ul>
<li>先二分得出槽中间位是 (0+4)&#x2F;2&#x3D;2 ，2号槽里最大的记录为 8。因为 10 &gt; 8，所以需要从 2 号槽后继续搜索记录</li>
<li>再使用二分搜索出 2 号和 4 槽的中间位是 (2+4)&#x2F;2&#x3D; 3，3 号槽里最大的记录为 12。因为 10 &lt; 12，所以键值为 10 的记录在 3 号槽里</li>
<li>再从 3 号槽指向的主键值为 9 记录开始向下搜索 1 次，定位到主键为 10 的记录，取出该条记录的信息即为我们想要查找的内容</li>
</ul>
<p>此外，还必须清楚：</p>
<ul>
<li>B+树 索引本身并不能找到具体的一条记录，能找到的只是该记录所在的页。数据库把页载入内存，然后通过 Page Directory 进行二叉查找。只不过二叉查找的时间复杂度很低，同时内存中的查找很快，因此通常忽略这部分查找所用的时间。</li>
</ul>
<h1 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h1><h2 id="二分查找法"><a href="#二分查找法" class="headerlink" title="二分查找法"></a>二分查找法</h2><p>二分查找法（binary search）也称为折半查找法，先以有序数列的中点位比较对象，如果要找的元素值小于该中心元素，则将待查找序列缩小为左半部分，否则为右半部分。通过一次比较，将查找区间缩小一半。如下图所示。</p>
<img src="/2022/10/13/mysql/mysql-index/half-search.png" class="" title="img.png">

<p>前面数据页结构中，每页 Page Directory 中的槽是按照主键顺序存放的，对于某一条具体记录的查询是通过对 Page Directory 进行二分查找得到的。</p>
<h2 id="二叉查找树"><a href="#二叉查找树" class="headerlink" title="二叉查找树"></a>二叉查找树</h2><p>二叉查找树不同于普通二叉查找树，是将小于根节点的元素放在左子树，而右子树正好相反，是放大于根节点的元素。如下图</p>
<img src="/2022/10/13/mysql/mysql-index/binary-tree.png" class="" title="img.png">

<p>二叉查找树在在查找数据时，只需要将需要查找的元素与树节点元素进行比较，当元素大于根节点则往右子树中查找，元素小于根节点则往左子树中查找，元素如果正好是中位数那么就是正好是根节点，所以二叉查找树具备高效查询。</p>
<p>但是二叉树也有明显弊端，在极端情况下，如果每次插入的数据都是最小或者都是最大的元素，那么树结构会退化成一条链表，如下图所示。</p>
<img src="/2022/10/13/mysql/mysql-index/binary-tree2.png" class="" title="img.png">

<p>当二叉查找树退化成链表时，查找效率很低，就和顺序查找差不多。如果想最大性能地构造出一个二叉树，需要这棵二叉查找树是平衡的。</p>
<h2 id="平衡二叉树"><a href="#平衡二叉树" class="headerlink" title="平衡二叉树"></a>平衡二叉树</h2><p>平衡二叉树是来解决二叉查找树极端情况下退化为链表的问题。平衡二叉树其实就是在二叉查找树的基础上加上约束：让每个节点的左右子树高度差不能超过 1。这样可以让左右子树都保持平衡。如下图。</p>
<img src="/2022/10/13/mysql/mysql-index/balance-binary-tree.png" class="" title="img.png">

<p>但是尽管是平衡二叉树，也会随着插入的元素增多，而导致树的高度变高，这同样意味着磁盘 I&#x2F;O 操作次数变多，影响到整体的查询效率。</p>
<h2 id="B-树"><a href="#B-树" class="headerlink" title="B 树"></a>B 树</h2><p>平衡二叉树本身是一个二叉树，每个节点只能有2个子节点，随着数据量的增大，节点个数越多，树的高度也会增高，增加了磁盘的 I&#x2F;O 次数，影响查询效率。</p>
<p>B 树的出现可以解决树高度的问题。之所以是 B 树，而不是”某某二叉树”，就是它不在限制一个父节点中只能有两个子节点，而是允许 M 个子节点（M&gt;<br>2）。不仅如此，B树的一个节点可以存储多个元素，相比较于前面的那些二叉树数据结构又将整体的树高度降低了。</p>
<p>B 树是一棵多叉树，它的每一个节点包含的最多子节点数量称为B树的阶。下图是一棵3阶的B树。</p>
<img src="/2022/10/13/mysql/mysql-index/b-tree.png" class="" title="img.png">

<ul>
<li>每个节点称为页，在mysql中数据读取去的基本单位是页，而也就是上面的磁盘块。P节点是指向子节点的指针。</li>
</ul>
<h3 id="B-树查找流程"><a href="#B-树查找流程" class="headerlink" title="B 树查找流程"></a>B 树查找流程</h3><p>在这个3阶B树中，查找 89 这个元素时的流程：</p>
<p>先从根节点出发，也就是 磁盘块1，判断 89 大于 45，通过磁盘块1中的指针 p3 找到磁盘块4。还是按照原来的步骤，在磁盘块4中的65 ~ 87之间相比较，最后磁盘4的指针p3找到磁盘块11。也就找到有匹配89的键值。</p>
<p>B树其实已经满足了减少磁盘 I&#x2F;O 操作，同时支持按区间查找。但是 B树的区间查找效率并不高。因为B树在做范围查询时，需要使用中序遍历，那么父节点和子节点也就需要不断的来回切换。会给磁盘 I&#x2F;O 带来很多负担。</p>
<h2 id="B-树-1"><a href="#B-树-1" class="headerlink" title="B+ 树"></a>B+ 树</h2><p>B+树从 + 可以看出是B树的升级版，MySQL 中 InnoDB 引擎中的索引底层数据结构采用的正是 B+树。</p>
<h3 id="B-树结构"><a href="#B-树结构" class="headerlink" title="B+ 树结构"></a>B+ 树结构</h3><p>B+树相比于B树，做了这样的升级：B+树中的非叶子节点都不存储数据，而是只作为索引。由叶子节点存放整棵树的所有数据。而叶子节点之间构成一个从小到大有序的链表互相指向相邻的叶子节点，也就是叶子节点之间形成了有序的双向链表。如下图B+树的结构。</p>
<img src="/2022/10/13/mysql/mysql-index/b(+)-tree.png" class="" title="img.png">

<h3 id="B-树查询"><a href="#B-树查询" class="headerlink" title="B+ 树查询"></a>B+ 树查询</h3><p>B+ 树底层是数据，上层都是按底层区间构成的索引。搜到到关键字不会直接返回，会一直走到叶子节点这一层查询数据。比如搜索 id&#x3D;10，虽然在根节点中就命中了，但是全部的数据在叶子节点上，所以还要继续往下搜索，一直到叶子节点。</p>
<h3 id="B-树插入和删除"><a href="#B-树插入和删除" class="headerlink" title="B+ 树插入和删除"></a>B+ 树插入和删除</h3><p>B树没有冗余节点，删除节点时会发生复杂的树变形。B+树做了大量冗余节点，从上面可以发现父节点的所有元素都会在子节点中出现，这样当删除一个节点时，可以直接从叶子节点中删除，这样效率更快，不会涉及到复杂的树变形。</p>
<p>而且B+ 树的插入也是如此，最多只涉及树的一条分支路线。</p>
<h3 id="估算一颗-B-树-中的数据量"><a href="#估算一颗-B-树-中的数据量" class="headerlink" title="估算一颗 B+ 树 中的数据量"></a>估算一颗 B+ 树 中的数据量</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">假设一条记录是 <span class="number">1</span>K，一个叶子节点（数据页）可以存储 <span class="number">16</span> 条记录，那非叶子节点可以存储多少个指针呢？</span><br><span class="line">假设索引字段是 <span class="type">bigint</span> 类型，长度是 <span class="number">8</span> 字节。指针大小为 <span class="number">6</span> 字节，这样一共 <span class="number">14</span> 字节。非叶子节点（索引页）可以存储 <span class="number">16384</span><span class="operator">/</span><span class="number">14</span> <span class="operator">=</span><span class="number">1170</span> 个这样的单元（键值<span class="operator">+</span>指针）。</span><br><span class="line">树深度为<span class="number">2</span>的时候，有 <span class="number">1170</span><span class="operator">^</span><span class="number">2</span> 个叶子节点，<span class="number">1170</span><span class="operator">^</span><span class="number">2</span><span class="operator">*</span><span class="number">16</span><span class="operator">=</span><span class="number">21902400</span>。</span><br><span class="line">在查询数据时一次页的查找代表一次 IO，也就是说，一张 <span class="number">2000</span> 万数据的表，查询数据最多需要访问<span class="number">3</span>次磁盘。所以 在 InnoDB 中 B<span class="operator">+</span> 树深度一般为 <span class="number">1</span><span class="number">-3</span> 层，就能满足千万级别的数据存储。</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="Explain"><a href="#Explain" class="headerlink" title="Explain"></a>Explain</h1><p><strong>explain</strong> 关键字可以模拟 MySQL 优化器执行 SQL 语句，可以很好的分析 SQL 语句或表结构的性能瓶颈。</p>
<h2 id="key-len-计算公式"><a href="#key-len-计算公式" class="headerlink" title="key_len 计算公式"></a>key_len 计算公式</h2><table>
<thead>
<tr>
<th>字段类型</th>
<th>长度</th>
<th>latin1</th>
<th>gbk</th>
<th>uft8</th>
<th>utf8mb4</th>
<th>允许为 null</th>
<th>不允许为 null</th>
<th>key_len</th>
</tr>
</thead>
<tbody><tr>
<td>char</td>
<td>L</td>
<td>k &#x3D; 1</td>
<td>k &#x3D; 2</td>
<td>k &#x3D; 3</td>
<td>k &#x3D; 4</td>
<td>n &#x3D; 1</td>
<td>n &#x3D; 0</td>
<td>L*k + n</td>
</tr>
<tr>
<td>varchar</td>
<td>L</td>
<td>k &#x3D; 1</td>
<td>k &#x3D; 2</td>
<td>k &#x3D; 3</td>
<td>k &#x3D; 4</td>
<td>n &#x3D; 1</td>
<td>n &#x3D; 0</td>
<td>L*k + n + 2</td>
</tr>
<tr>
<td>tinyint</td>
<td>1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>n &#x3D; 1</td>
<td>n &#x3D; 0</td>
<td>1 + n</td>
</tr>
<tr>
<td>smallint</td>
<td>2</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>n&#x3D;1</td>
<td>n &#x3D; 0</td>
<td>2 + n</td>
</tr>
<tr>
<td>mediumint</td>
<td>3</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>n&#x3D;1</td>
<td>n &#x3D; 0</td>
<td>3 + n</td>
</tr>
<tr>
<td>int</td>
<td>4</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>n&#x3D;1</td>
<td>n &#x3D; 0</td>
<td>4 + n</td>
</tr>
<tr>
<td>bigint</td>
<td>8</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>n&#x3D;1</td>
<td>n &#x3D; 0</td>
<td>8 + n</td>
</tr>
<tr>
<td>datetime</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>(mysql5.6及以后)</td>
<td>5</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>n&#x3D;1</td>
<td>n &#x3D; 0</td>
<td>5 + n</td>
</tr>
<tr>
<td>date</td>
<td>3</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>n&#x3D;1</td>
<td>n &#x3D; 0</td>
<td>3 + n</td>
</tr>
<tr>
<td>time</td>
<td>3</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>n&#x3D;1</td>
<td>n &#x3D; 0</td>
<td>3 + n</td>
</tr>
<tr>
<td>year</td>
<td>1</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>n&#x3D;1</td>
<td>n &#x3D; 0</td>
<td>1 + n</td>
</tr>
<tr>
<td>timestamp</td>
<td>4</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>n&#x3D;1</td>
<td>n &#x3D; 0</td>
<td>4 + n</td>
</tr>
</tbody></table>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>整数类型，浮点数类型，时间类型的索引长度 <span class="keyword">NOT</span> <span class="keyword">NULL</span><span class="operator">=</span>字段本身的字段长度 <span class="keyword">NULL</span><span class="operator">=</span>字段本身的字段长度<span class="operator">+</span><span class="number">1</span>，因为需要有是否为空的标记，这个标记需要占用<span class="number">1</span>个字节 datetime 类型在<span class="number">5.6</span>中字段长度是<span class="number">5</span>个字节</span><br><span class="line"></span><br><span class="line"><span class="number">2.</span>字符类型 varchr(n)变长字段且允许<span class="keyword">NULL</span> <span class="operator">=</span> n <span class="operator">*</span> (utf8mb4<span class="operator">=</span><span class="number">4</span>,utf8<span class="operator">=</span><span class="number">3</span>,gbk<span class="operator">=</span><span class="number">2</span>,latin1<span class="operator">=</span><span class="number">1</span>)<span class="operator">+</span><span class="number">1</span>(<span class="keyword">NULL</span>)<span class="operator">+</span><span class="number">2</span> varchr(n)变长字段且不允许<span class="keyword">NULL</span> <span class="operator">=</span> n <span class="operator">*</span> (</span><br><span class="line">utf8mb4<span class="operator">=</span><span class="number">4</span>,utf8<span class="operator">=</span><span class="number">3</span>,gbk<span class="operator">=</span><span class="number">2</span>,latin1<span class="operator">=</span><span class="number">1</span>)<span class="operator">+</span><span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="type">char</span>(n)固定字段且允许<span class="keyword">NULL</span> <span class="operator">=</span> n <span class="operator">*</span> (utf8mb4<span class="operator">=</span><span class="number">4</span>,utf8<span class="operator">=</span><span class="number">3</span>,gbk<span class="operator">=</span><span class="number">2</span>,latin1<span class="operator">=</span><span class="number">1</span>)<span class="operator">+</span><span class="number">1</span>(<span class="keyword">NULL</span>)</span><br><span class="line"><span class="type">char</span>(n)固定字段且允许<span class="keyword">NULL</span> <span class="operator">=</span> n <span class="operator">*</span> (utf8mb4<span class="operator">=</span><span class="number">4</span>,utf8<span class="operator">=</span><span class="number">3</span>,gbk<span class="operator">=</span><span class="number">2</span>,latin1<span class="operator">=</span><span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">变长字段需要额外的<span class="number">2</span>个字节来记录长度，所以VARCAHR索引长度计算时候要加<span class="number">2</span>。固定长度字段不需要额外的字节。而<span class="keyword">null</span>都需要<span class="number">1</span>个字节的额外空间,所以索引字段最好不要为<span class="keyword">NULL</span>，因为<span class="keyword">NULL</span>让统计更加复杂，并且需要额外的存储空间。这个结论在此得到了证实，复合索引有最左前缀的特性，如果复合索引能全部使用上，则是复合索引字段的索引长度之和，这也可以用来判定复合索引是否部分使用，还是全部使用。</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>##explain 各字段含义<br>explain 中各字段含义见下图：</p>
<img src="/2022/10/13/mysql/mysql-index/explain.png" class="" title="img.png">
<p>#索引</p>
<p>##索引定义</p>
<p>索引是一个单独的、存储在磁盘上的数据库结构，包含着对数据表里多有记录的引用指针。使用索引可以快度找出某个或多个列中有一特定值的行，所有 MySQL 列类型都可以被索引，对相关列使用索引是提高查询操作速度的最佳途径。<br>##聚集索引和非聚集索引</p>
<p>###概括</p>
<p>聚集索引就是以 主键 创建的索引</p>
<p>非聚集索引就是以 非主键 创建的索引，也叫做 二级索引</p>
<p>###区别</p>
<ul>
<li>聚集索引在叶子节点存储的是表中的数据</li>
<li>非聚集索引在叶子节点存储的是主键和索引列</li>
<li>使用非聚集索引查询出数据时，先查询到叶子节点上的主键，再去主键索引中查找要找的数据(拿到主键再查找的这个过程叫做回表)</li>
</ul>
<p>##联合索引</p>
<p>联合索引是指按一定顺序对表上的多个列进行索引。一个联合索引是一个有序元组 &lt;a1, a2, …, an&gt;。单列索引可以看成联合索引元素数为1的特例。</p>
<p>###好处</p>
<ul>
<li>对于需要排序的查询，联合索引中的键值都是排序的，通过叶子节点可以逻辑上顺序读的读出所有数据，可以避免多一次的排序操作。</li>
</ul>
<p>###最左匹配原则</p>
<p>假如索引列分别为 A, B, C，且顺序也为 A, B, C</p>
<ul>
<li>如果查询的时候，查询 【A】【A, B】【A, B, C】，那么可以通过索引查询。</li>
<li>如果查询的时候，采用 【A, C】，那么 C 虽然是索引，但是由于中间缺失了B，因此C 这个索引是用不到的，只能用到 A 索引。</li>
<li>如果查询的时候，采用【B】【B, C】【C】，由于没有用到第一列索引，不是最左前缀，那么后面索引也是用不到的。</li>
<li>如果查询的时候，采用范围查询(&gt;、&lt;、between、like 左匹配)，并且是最左前缀，mysql 无法再使用范围列后面的其他索引列了。如果查询的时候，采用 A &#x3D; 1 and b &gt; 2 and C &#x3D; 3，则会在每个节点依次命中<br>A、B，无法命中C。</li>
<li>列的排列顺序决定了可命中索引的列数。</li>
</ul>
<p>###&#x3D;、in 自动优化顺序</p>
<p>不需要考虑&#x3D;、in 等的顺序，mysql 会自动优化这些条件的顺序，以匹配尽可能多的索引列。</p>
<ul>
<li>如有索引 (a, b, c, d)，查询条件 c &gt; 3 and b &#x3D; 2 and a &#x3D; 1 and d &lt; 4 与 a &#x3D; 1 and c &gt; 3 and b &#x3D; 2 and d &lt; 4 等顺序都是可以的，MySQL<br>都会自动优化为 a &#x3D; 1 and b &#x3D; 2 and c &gt; 3 and d &lt; 4，依次命 a、b、c。</li>
</ul>
<p>##覆盖索引</p>
<p>在联合索引中，存在着一种特殊的索引 - 覆盖索引。</p>
<p>###优势</p>
<ul>
<li>非聚集索引的叶子节点存储的是主键 + 列值，最终还是要 回表，也就是要通过主键再查找一次，这样就会比较慢。覆盖索引从二级索引中就可以得到查询的记录，不需要查询聚集索引中的记录。</li>
<li>覆盖索引不包含整行记录的所有信息，故其大小要远小于聚集索引，因此可以减少大量的 I&#x2F;O 操作。</li>
<li>能使用覆盖索引就使用。</li>
</ul>
<p>###举例</p>
<ul>
<li>如果现有索引 (username, age)，在查询数据的时候：select username，age from user where username &#x3D; ‘Tom’ and age &#x3D; 19;</li>
<li>很明显，where 后面的 username 和 age 是要走索引的，而且要查询的 username 和 age 也正是索引的列，这些列都存于索引的叶子节点上，所以就不用回表了。</li>
</ul>
<p>##使用索引扫描来做排序</p>
<p>MySQL 有两种方式可以生成有序的结果：通过排序操作 或者 按索引顺序扫描。如果 EXPLAIN 出来的 type 列的值为 “index”，则说明 MySQL 使用了索引扫描来做排序。</p>
<p>只有当索引的列顺序和 ORDER BY 子句的顺序完全一致，并且所有列的排序方向（倒序或顺序）都一样时，MySQL 才能够使用索引来对结果做排序。如果查询需要关联多张表，则只有当 ORDER BY<br>子句引用的字段全部为第一个表时，才能使用索引做排序。ORDER BY 子句和查找型查询的限制是一样的：需要满足索引的最左前缀的要求；否则，MySQL 都需要执行文件排序操作，而无法利用索引排序。</p>
<p>##索引不是越多越好</p>
<p>在进行插入操作时，数据页的存放还是按主键 id<br>的执行顺序存放，凡是对于非聚集索引，叶子节点的插入不再是顺序的了。这时就需要离散地访问非聚集索引页，插入性能在这里变低了。然而这并不是字段上索引的错误，因为B+树的特性决定了非聚集索引插入的离散型。</p>
<p>##索引下推</p>
<p>Index Condition Pushdown 是 MySQL 5.6 开始支持的一种根据索引进行查询的优化方式。mysql 数据库会在取出索引的同时，判断是否可以进行 where 条件的过滤，也就是将 where<br>的部分过滤操作放在了存储引擎层。在某些查询下，可以大大减少上层 sql 层对记录的索取（fetch），从而提高数据库的整体性能。</p>
<p>索引下推优化支持 range、ref、eq_ref、ref_or_null 类型的查询。支持 MyISAM 和 InnoDB 存储引擎。当优化器选择 Index Condition Pushdown 优化时，可以在执行计划的列 Extra<br>看到 Using index condition 提示。</p>
<p>##索引不可见</p>
<p>不可见索引是 mysql8.0 的新特性。</p>
<p>在删除多余索引前，可以先隐藏一个索引，然后观察对数据库的影响。如果数据库心梗有所下降，就说明这个索引是有用的，于是将其”恢复显示” 即可。如果数据库性能看不出变化，说明这个索引是多余的，可以删除了。</p>
<p>##索引倾斜</p>
<p>一般情况下，推荐在基数高的列上创建索引；比如订单表的status字段，可能该字段有出货中，派件中，已完成三个值，一般都会觉得在该列不推荐创建索引；但是在实际情况中，比如订单表有1000万条数据，只有10w条数据的status是派件中，而实际业务中，查询派件中的订单这种需求比较多，此时就可以创建status字段的索引了，因为status<br>列的值分布不均，这个索引是严重倾斜的，而索引的优势就是从大量数据中找出少量数据；但是就算创建了该列的索引，mysql的优化器可能也不会用该索引，因为优化器不会知道该列存在索引倾斜，此时有可能需要人工指定索引了，explain select</p>
<ul>
<li>from table force index() where …</li>
</ul>
<p>查看一个索引是否是高选择性，也就是基数（cardinality）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> information_schema.statistics <span class="keyword">where</span> table_schema<span class="operator">=</span><span class="string">&#x27;DBNAME&#x27;</span> <span class="keyword">and</span> table_name <span class="operator">=</span> <span class="string">&#x27;TABLENAME&#x27;</span>;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>在实际应用中，cardinality&#x2F;n_rows_in_table 应尽可能的接近1。如果非常小，那么需要考虑是否还有必要创建这个索引。</p>
<p>创建复合索引规则：应该把基数高的列放在前面，这样就可以最先筛选掉大部分的数据。</p>
<p>##索引失效的情况</p>
<ul>
<li>查询条件包含 or，可能导致索引失效 示例</li>
<li>如果字段名类型是字符串，where 时一定要用引号括起来,否则不使用索引 示例</li>
<li>使用 like 以 % 开头导致索引失效 示例</li>
<li>不符合最左匹配原则 示例</li>
<li>在索引列上使用 mysql 内置函数，索引失效 示例</li>
<li>对索引列运算（如，+、-、*、&#x2F;），索引失效 示例</li>
<li>mysql 估计使用全表扫描要比使用索引快,则不使用索引 示例</li>
<li>连接查询关联字段的字符集不一样，可能导致索引失效 示例</li>
</ul>
<p>##建立索引技巧</p>
<ul>
<li>最左匹配原则。MySQL 会一直向右匹配到范围查询 列的排列顺序决定了可命中索引的列数 就停止匹配</li>
<li>尽量选择区分度搞的列作为索引，区分度的公式是 COUNT(DISTINCT col) &#x2F; COUNT(*)，表示字段不重复的比率，比率越大扫描的记录数就越少。</li>
<li>索引不能参与计算，尽量保持列“干净”。比如 比如，FROM_UNIXTIME(create_time) &#x3D; ‘2022-08-06’<br>就不能使用索引，原因很简单，B+树中存储的都是数据表中的字段值，但是进行检索时，需要把所有元素都应用函数才能比较，显然这样的代价太大。所以语句要写成 ： create_time &#x3D; UNIX_TIMESTAMP(‘<br>2022-08-06’)</li>
<li>尽可能的扩展索引，不要新建立索引。比如表中已经有了 (a) 的索引，现在要加 (a, b) 的索引，那么只需要修改原来的索引即可。</li>
<li>单个多列组合索引和多个单列索引的检索查询效果不同，因为在执行SQL时，MySQL会从多个单列索引中选择一个或多个（union 索引合并时）效率最高的索引。</li>
</ul>
<p>索引举例</p>
<p>创建 t_user_action_log 表并插入数据</p>
<p>– create table CREATE TABLE <code>customer_info</code> (<br><code>id</code> bigint(20) unsigned NOT NULL AUTO_INCREMENT,<br><code>name</code> varchar(32) NOT NULL COMMENT ‘名字’,<br><code>province</code> varchar(32) NOT NULL COMMENT ‘省份’,<br><code>level</code> int(6) NOT NULL COMMENT ‘等级’,<br><code>sex</code> varchar(10) NOT NULL COMMENT ‘性别’, PRIMARY KEY (<code>id</code>)<br>) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8mb4 ROW_FORMAT&#x3D;DYNAMIC COMMENT&#x3D;’顾客信息表’;</p>
<p>– create sql script import random import string</p>
<p>with open(‘customer.sql’, ‘a’) as f:<br>for i in range(0, 100000):<br>name&#x3D;’’.join(random.sample(string.ascii_lowercase, 4))</p>
<pre><code>    provice = random.choice([&#39;heilongjiang&#39;, &#39;jilin&#39;, &#39;liaoning&#39;, &#39;hebei&#39;, &#39;neimenggu&#39;, &#39;henan&#39;, &#39;xinjiang&#39;, &#39;gansu&#39;, &#39;shanxi&#39;, &#39;shanxi&#39;, &#39;shandong&#39;, &#39;jiangsu&#39;, &#39;hubei&#39;, &#39;sichuan&#39;, &#39;xizang&#39;, &#39;qinghai&#39;, &#39;anhui&#39;, &#39;jiangsu&#39;, &#39;zhejiang&#39;, &#39;jiangxi&#39;, &#39;fujian&#39;, &#39;hunan&#39;, &#39;guzhou&#39;, &#39;yunan&#39;, &#39;guangxi&#39;, &#39;guangdong&#39;, &#39;taiwan&#39;, &#39;hainan&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;, &#39;jilin&#39;])

    level = random.choice([1, 2, 3, 4, 5, 6, 7, 8, 9, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])

    sex = random.choice([&#39;female&#39;, &#39;male&#39;, &#39;female&#39;, &#39;female&#39;, &#39;female&#39;, &#39;female&#39;, &#39;female&#39;, &#39;female&#39;, &#39;female&#39;, &#39;female&#39;, &#39;female&#39;])

    sql = f&quot;INSERT INTO customer_info (name, province, level, sex) values (&#39;&#123;name&#125;&#39;, &#39;&#123;provice&#125;&#39;, &#39;&#123;level&#125;&#39;, &#39;&#123;sex&#125;&#39;);&quot;


    f.write(sql)
    f.write(&#39;\n&#39;)
</code></pre>
<p>– impport sql 将数据导入表中</p>
<p>1，主键索引</p>
<p>查询 id 为 1 的记录，sql 如下：</p>
<p>select name, province, level, sex from customer_info where id &#x3D; 1;</p>
<p>通过查询分析器explain分析这条查询语句：</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where id &#x3D; 1;<br>+—-+————-+—————+————+——-+—————+———+———+——-+——+———-+——-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——-+—————+———+———+——-+——+———-+——-+<br>| 1 | SIMPLE | customer_info | NULL | const | PRIMARY | PRIMARY | 8 | const | 1 | 100.00 | NULL |<br>+—-+————-+—————+————+——-+—————+———+———+——-+——+———-+——-+</p>
<ul>
<li>key 都为 PRIMARY，表示使用了主键索引</li>
<li>ref 为 const，表示通过索引一次就找到了</li>
<li>rows 为 1，表示大致估算出找到所需的记录需要读取1行记录</li>
</ul>
<p>2，非主键单列索引</p>
<p>添加索引前后对比</p>
<p>查询 province 为 zhejiang的所有记录，sql 如下：</p>
<p>select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’;</p>
<p>通过查询分析器explain分析这条查询语句：</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’;<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| 1 | SIMPLE | customer_info | NULL | ALL | NULL | NULL | NULL | NULL | 100962 | 10.00 | Using where |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>type为ALL表示要进行全表扫描。这样效率无疑是极慢的。</li>
</ul>
<p>为province列添加索引：</p>
<p>alter table customer_info add index prov_idx(<code>province</code>);</p>
<p>再通过查询分析器分析：</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’;<br>+—-+————-+—————+————+——+—————+———-+———+——-+——+———-+——-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+—————+———-+———+——-+——+———-+——-+<br>| 1 | SIMPLE | customer_info | NULL | ref | prov_idx | prov_idx | 130 | const | 1768 | 100.00 | NULL |<br>+—-+————-+—————+————+——+—————+———-+———+——-+——+———-+——-+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>看到这次查询就使用索引 prov_idx 了。</li>
<li>这次查询扫描了 1768 行，即所有 province&#x3D; ‘zhejiang’ 的行。</li>
</ul>
<p>varchar 中存的为数字时</p>
<p>insert into customer_info(<code>name</code>, <code>province</code>, <code>level</code>, <code>sex</code>) values (‘kd’, ‘1’, 2, ‘male’);</p>
<p>通过查询分析器explain分析</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; 1;<br>+—-+————-+—————+————+——+——————–+——+———+——+——–+———-+————-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——+———+——+——–+———-+————-+<br>| 1 | SIMPLE | customer_info | NULL | ALL | prov_level_sex_idx | NULL | NULL | NULL | 100963 | 10.00 | Using where |<br>+—-+————-+—————+————+——+——————–+——+———+——+——–+———-+————-+<br>1 row in set, 3 warnings (0.00 sec)</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘1’;<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+——-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+——-+<br>| 1 | SIMPLE | customer_info | NULL | ref | prov_level_sex_idx | prov_level_sex_idx | 130 | const | 1 | 100.00 | NULL |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+——-+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>如果字段名类型是字符串，where 时一定要用引号括起来,否则不使用索引</li>
</ul>
<p>在索引列上使用 mysql 内置函数</p>
<p>alter table customer_info add key lvl_idx(<code>level</code>);</p>
<p>通过查询分析器explain分析</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where level &#x3D; 5;<br>+—-+————-+—————+————+——+—————+———+———+——-+——+———-+——-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+—————+———+———+——-+——+———-+——-+<br>| 1 | SIMPLE | customer_info | NULL | ref | lvl_idx | lvl_idx | 4 | const | 2179 | 100.00 | NULL |<br>+—-+————-+—————+————+——+—————+———+———+——-+——+———-+——-+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where ABS(level) &#x3D; 5;<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| 1 | SIMPLE | customer_info | NULL | ALL | NULL | NULL | NULL | NULL | 100963 | 100.00 | Using where |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>在索引列上使用 mysql 内置函数，索引失效</li>
</ul>
<p>对索引列进行运算</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where level &#x3D; 5;<br>+—-+————-+—————+————+——+—————+———+———+——-+——+———-+——-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+—————+———+———+——-+——+———-+——-+<br>| 1 | SIMPLE | customer_info | NULL | ref | lvl_idx | lvl_idx | 4 | const | 2179 | 100.00 | NULL |<br>+—-+————-+—————+————+——+—————+———+———+——-+——+———-+——-+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where level - 1 &#x3D; 4;<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| 1 | SIMPLE | customer_info | NULL | ALL | NULL | NULL | NULL | NULL | 100963 | 100.00 | Using where |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>对索引列运算（如，+、-、*、&#x2F;），索引失效</li>
</ul>
<p>可以使用索引但未走索引</p>
<p>mysql&gt; explain select name, level from customer_info where level not in (5);<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| 1 | SIMPLE | customer_info | NULL | ALL | lvl_idx | NULL | NULL | NULL | 100963 | 58.75 | Using where |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>mysql&gt; explain select name, level from customer_info force index(lvl_idx) where level not in (5);<br>+—-+————-+—————+————+——-+—————+———+———+——+——-+———-+———————–+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——-+—————+———+———+——+——-+———-+———————–+<br>| 1 | SIMPLE | customer_info | NULL | range | lvl_idx | lvl_idx | 4 | NULL | 59316 | 100.00 | Using index condition |<br>+—-+————-+—————+————+——-+—————+———+———+——+——-+———-+———————–+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>mysql 估计使用全表扫描要比使用索引快,则不使用索引</li>
</ul>
<p>3，联合索引</p>
<p>删除之前创建的索引 prov_idx ， lvl_idx</p>
<p>alter table customer_info dorp index prov_idx, drop index lvl_idx;</p>
<p>查询条件：来自浙江，level 为 1 的男生。sql 如下：</p>
<p>select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’ and level&#x3D;1 and sex&#x3D;’male’;</p>
<p>通过查询分析器explain分析这条查询语句：</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’ and level&#x3D;1 and<br>sex&#x3D;’male’;<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| 1 | SIMPLE | customer_info | NULL | ALL | NULL | NULL | NULL | NULL | 100962 | 0.10 | Using where |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>type 为 ALL，全表扫描</li>
</ul>
<p>创建一个联合索引</p>
<p>alter table customer_info add index prov_level_sex_idx(<code>province</code>, <code>level</code>, <code>sex</code>);</p>
<p>再查询分析器分析：</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’ and level&#x3D;1 and<br>sex&#x3D;’male’;<br>+—-+————-+—————+————+——+——————–+——————–+———+——————-+——+———-+——-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——————–+———+——————-+——+———-+——-+<br>| 1 | SIMPLE | customer_info | NULL | ref | prov_level_sex_idx | prov_level_sex_idx | 176 | const,const,const | 126 |<br>100.00 | NULL |<br>+—-+————-+—————+————+——+——————–+——————–+———+——————-+——+———-+——-+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>查询走了索引 prov_sex_level_idx</li>
<li>ref 显示命中了 prov_sex_level_idx 中的三个字段</li>
</ul>
<p>更换查询条件顺序</p>
<p>更换 where 后面的查询条件再执行一次 explain：</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where sex&#x3D;’male’ and province &#x3D; ‘zhejiang’ and<br>level&#x3D;1;<br>+—-+————-+—————+————+——+——————–+——————–+———+——————-+——+———-+——-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——————–+———+——————-+——+———-+——-+<br>| 1 | SIMPLE | customer_info | NULL | ref | prov_level_sex_idx | prov_level_sex_idx | 176 | const,const,const | 126 |<br>100.00 | NULL |<br>+—-+————-+—————+————+——+——————–+——————–+———+——————-+——+———-+——-+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>结果和上面的一样，说明 mysql 会自动优化这些条件的顺序</li>
</ul>
<p>查询条件变为2个</p>
<p>– 查询条件为 province 和 level mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘<br>zhejiang’ and level&#x3D;1;<br>+—-+————-+—————+————+——+——————–+——————–+———+————-+——+———-+——-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——————–+———+————-+——+———-+——-+<br>| 1 | SIMPLE | customer_info | NULL | ref | prov_level_sex_idx | prov_level_sex_idx | 134 | const,const | 1486 | 100.00<br>| NULL |<br>+—-+————-+—————+————+——+——————–+——————–+———+————-+——+———-+——-+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>– 查询条件为 province 和 sex mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’<br>and sex&#x3D;’male’;<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+———————–+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+———————–+<br>| 1 | SIMPLE | customer_info | NULL | ref | prov_level_sex_idx | prov_level_sex_idx | 130 | const | 1768 | 10.00 | Using<br>index condition |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+———————–+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>– 查询条件为 level 和 sex mysql&gt; explain select name, province, level, sex from customer_info where level&#x3D;1 and sex&#x3D;’male’;<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| 1 | SIMPLE | customer_info | NULL | ALL | NULL | NULL | NULL | NULL | 100962 | 1.00 | Using where |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>查询条件为 province 和 level 时，符合最左前缀原则，走了 prov_sex_level_idx 索引</li>
<li>查询条件为 province 和 sex 时，只有 province 符合最左匹配原则，所以只有 province 走了索引</li>
<li>查询条件为 level 和 sex 时，都不符合最左匹配原则，不能走索引</li>
</ul>
<p>查询条件变为 1 个</p>
<p>– 查询条件为 province mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’;<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+——-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+——-+<br>| 1 | SIMPLE | customer_info | NULL | ref | prov_level_sex_idx | prov_level_sex_idx | 130 | const | 1768 | 100.00 | NULL<br>|<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+——-+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>– 查询条件为 level mysql&gt; explain select name, province, level, sex from customer_info where level&#x3D;1;<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| 1 | SIMPLE | customer_info | NULL | ALL | NULL | NULL | NULL | NULL | 100962 | 10.00 | Using where |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>– 查询条件为 sex mysql&gt; explain select name, province, level, sex from customer_info where sex&#x3D;’male’;<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| 1 | SIMPLE | customer_info | NULL | ALL | NULL | NULL | NULL | NULL | 100962 | 10.00 | Using where |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>和预期的一样，只有查询条件为 province 时才符合最左前缀原则，才会走索引</li>
</ul>
<p>查询条件中有范围查询 &gt;、&lt;</p>
<p>– 范围查询 &gt; &lt;<br>mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’ and level &gt; 2 and<br>level &lt; 8;<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| 1 | SIMPLE | customer_info | NULL | range | prov_level_sex_idx | prov_level_sex_idx | 134 | NULL | 174 | 100.00 |<br>Using index condition |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’ and level &gt; 2 and<br>level &lt; 8 and sex&#x3D;’male’;<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| 1 | SIMPLE | customer_info | NULL | range | prov_level_sex_idx | prov_level_sex_idx | 134 | NULL | 174 | 10.00 | Using<br>index condition |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>使用了 prov_level_sex_idx 索引，根据扫描的行数知道，province 和 level 字段使用了索引</li>
<li>遇到 &gt;、&lt;范围查询，mysql 无法再使用范围列后面的其他索引列了</li>
</ul>
<p>查询条件中有 between</p>
<p>– 范围查询 between mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’ and<br>level between 3 and 7;<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| 1 | SIMPLE | customer_info | NULL | range | prov_level_sex_idx | prov_level_sex_idx | 134 | NULL | 174 | 100.00 |<br>Using index condition |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>mysql&gt;  explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’ and level between 3 and<br>7 and sex&#x3D;’male’;<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| 1 | SIMPLE | customer_info | NULL | range | prov_level_sex_idx | prov_level_sex_idx | 176 | NULL | 133 | 10.00 | Using<br>index condition |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>从 key 看，三个字段都走了索引，但是从 filter看，并不是所有的字段都走了索引。那该如何判断？ 可以看下这篇文章 The range access method and why you should use EXPLAIN<br>JSON，这时候 explain 可以加上参数 format&#x3D;json 来查看哪些字段走了索引</p>
<p>mysql&gt; explain format&#x3D;json select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’ and level<br>between 3 and 7 and sex&#x3D;’male’ \G<br>*************************** 1. row ***************************<br>EXPLAIN: {<br>“query_block”: {<br>“select_id”: 1,<br>“cost_info”: {<br>“query_cost”: “187.21”<br>},<br>“table”: {<br>“table_name”: “customer_info”,<br>“access_type”: “range”,<br>“possible_keys”: [<br>“prov_level_sex_idx”<br>],<br>“key”: “prov_level_sex_idx”,<br>“used_key_parts”: [<br>“province”,<br>“level”<br>],<br>“key_length”: “176”,<br>“rows_examined_per_scan”: 133,<br>“rows_produced_per_join”: 13,<br>“filtered”: “10.00”,<br>“index_condition”: “((<code>testdb</code>.<code>customer_info</code>.<code>province</code> &#x3D; ‘zhejiang’) and (<code>testdb</code>.<code>customer_info</code>.<code>level</code> between 3<br>and 7) and (<code>testdb</code>.<code>customer_info</code>.<code>sex</code> &#x3D; ‘male’))”,<br>“cost_info”: {<br>“read_cost”: “184.55”,<br>“eval_cost”: “2.66”,<br>“prefix_cost”: “187.21”,<br>“data_read_per_join”: “4K”<br>},<br>“used_columns”: [<br>“name”,<br>“province”,<br>“level”,<br>“sex”<br>]<br>} } } 1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>从 “used_key_parts”: [“province”, “level”] 可以看出只有 province 和 level 走了索引，sex 字段并没有走索引。</li>
<li>key_len 字段并不是所有走了索引的索引长度之和</li>
<li>使用 format&#x3D;json 可以更好的查看 explain</li>
<li>遇到 between 范围查询，mysql 无法再使用范围列后面的其他索引列了</li>
</ul>
<p>查询条件中有 like</p>
<p>– 范围查询 like mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’;<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+——-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+——-+<br>| 1 | SIMPLE | customer_info | NULL | ref | prov_level_sex_idx | prov_level_sex_idx | 130 | const | 1768 | 100.00 | NULL<br>|<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+——-+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province like ‘%zhe’;<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| 1 | SIMPLE | customer_info | NULL | ALL | NULL | NULL | NULL | NULL | 100962 | 11.11 | Using where |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province like ‘zhe%’;<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| 1 | SIMPLE | customer_info | NULL | range | prov_level_sex_idx | prov_level_sex_idx | 130 | NULL | 1768 | 100.00 |<br>Using index condition |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province like ‘zhe%iang’;<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| 1 | SIMPLE | customer_info | NULL | range | prov_level_sex_idx | prov_level_sex_idx | 130 | NULL | 1768 | 100.00 |<br>Using index condition |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province like ‘zhe%’ and level &#x3D; 9;<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| 1 | SIMPLE | customer_info | NULL | range | prov_level_sex_idx | prov_level_sex_idx | 134 | NULL | 1768 | 10.00 |<br>Using index condition |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li><p>前模糊不能使用索引，非前模糊可以使用索引</p>
</li>
<li><p>和 between 一样，这里通过 key_len 和 filter 判断哪些字段走了索引时有冲突，使用 format&#x3D;json 重新查看 mysql&gt; explain format&#x3D;json select name,<br>province, level, sex from customer_info where province like ‘zhe%’ and level &#x3D; 9 \G<br>*************************** 1. row ***************************<br>EXPLAIN: {<br>“query_block”: {<br>“select_id”: 1,<br>“cost_info”: {<br>“query_cost”: “2476.21”<br>},<br>“table”: {<br>“table_name”: “customer_info”,<br>“access_type”: “range”,<br>“possible_keys”: [<br>“prov_level_sex_idx”<br>],<br>“key”: “prov_level_sex_idx”,<br>“used_key_parts”: [<br>“province”<br>],<br>“key_length”: “134”,<br>“rows_examined_per_scan”: 1768,<br>“rows_produced_per_join”: 176,<br>“filtered”: “10.00”,<br>“index_condition”: “((<code>testdb</code>.<code>customer_info</code>.<code>level</code> &#x3D; 9) and (<code>testdb</code>.<code>customer_info</code>.<code>province</code> like ‘zhe%’))”,<br>“cost_info”: {<br>“read_cost”: “2440.85”,<br>“eval_cost”: “35.36”,<br>“prefix_cost”: “2476.21”,<br>“data_read_per_join”: “53K”<br>},<br>“used_columns”: [<br>“name”,<br>“province”,<br>“level”,<br>“sex”<br>]<br>} } } 1 row in set, 1 warning (0.00 sec)</p>
</li>
<li><p>从 “used_key_parts”: [“province”] 可以看出只有 province 走了索引，level 字段并没有走索引。</p>
</li>
<li><p>遇到 like 范围查询，mysql 无法再使用范围列后面的其他索引列了</p>
</li>
</ul>
<p>查询条件中有 or</p>
<p>– 相同列使用 or 查询 – drop index prov_level_sex_idx on customer_info; – alter table customer_info add index<br>prov(<code>province</code>); mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’ or<br>province&#x3D; ‘jiangsu’;<br>+—-+————-+—————+————+——-+—————+——+———+——+——+———-+———————–+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——-+—————+——+———+——+——+———-+———————–+<br>| 1 | SIMPLE | customer_info | NULL | range | prov | prov | 130 | NULL | 5280 | 100.00 | Using index condition |<br>+—-+————-+—————+————+——-+—————+——+———+——+——+———-+———————–+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>– 不同列且部分列建立索引 mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’ or level<br>&#x3D; 9;<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>| 1 | SIMPLE | customer_info | NULL | ALL | prov | NULL | NULL | NULL | 100962 | 19.00 | Using where |<br>+—-+————-+—————+————+——+—————+——+———+——+——–+———-+————-+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>– 不同列且全部列分别建立索引 – alter table customer_info add index level(<code>level</code>); mysql&gt; explain select name, province, level, sex<br>from customer_info where province &#x3D; ‘zhejiang’ or level &#x3D; 9;<br>+—-+————-+—————+————+————-+—————+————+———+——+——+———-+————————————–+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+————-+—————+————+———+——+——+———-+————————————–+<br>| 1 | SIMPLE | customer_info | NULL | index_merge | prov,level | prov,level | 130,4 | NULL | 3999 | 100.00 | Using<br>union(prov,level); Using where |<br>+—-+————-+—————+————+————-+—————+————+———+——+——+———-+————————————–+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>– 不同列建立联合索引 – drop index prov on customer_info; – drop index level on customer_info; – alter table customer_info add<br>index prov_level_sex_idx(<code>province</code>, <code>level</code>, <code>sex</code>); mysql&gt; explain select name, province, level, sex from<br>customer_info where province &#x3D; ‘zhejiang’ or level &#x3D; 9;<br>+—-+————-+—————+————+——+——————–+——+———+——+——–+———-+————-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——+———+——+——–+———-+————-+<br>| 1 | SIMPLE | customer_info | NULL | ALL | prov_level_sex_idx | NULL | NULL | NULL | 100962 | 19.00 | Using where |<br>+—-+————-+—————+————+——+——————–+——+———+——+——–+———-+————-+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>相同列使用 or：索引生效</li>
<li>不同列且部分列建立索引：索引失效</li>
<li>不同列且全部列分别建立索引：自动将 or 转为 union，索引生效</li>
<li>不同列建立联合索引：索引失效</li>
</ul>
<p>查询条件中有 in</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province in (‘zhejiang’, ‘jiangsu’);<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| 1 | SIMPLE | customer_info | NULL | range | prov_level_sex_idx | prov_level_sex_idx | 130 | NULL | 5280 | 100.00 |<br>Using index condition |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province in (‘zhejiang’, ‘jiangsu’) and level<br>&#x3D; 9;<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>| 1 | SIMPLE | customer_info | NULL | range | prov_level_sex_idx | prov_level_sex_idx | 134 | NULL | 110 | 100.00 |<br>Using index condition |<br>+—-+————-+—————+————+——-+——————–+——————–+———+——+——+———-+———————–+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>in 后面的 level 字段可以走索引</li>
</ul>
<p>查询条件中有 order by</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’;<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+——-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+——-+<br>| 1 | SIMPLE | customer_info | NULL | ref | prov_level_sex_idx | prov_level_sex_idx | 130 | const | 1768 | 100.00 | NULL<br>|<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+——-+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>– 索引列顺序和 order by 子句的顺序完全一致 – 所有列的排序方向全部相同 mysql&gt; explain select name, province, level, sex from customer_info where<br>province &#x3D; ‘zhejiang’ order by level,sex;<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+———————–+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+———————–+<br>| 1 | SIMPLE | customer_info | NULL | ref | prov_level_sex_idx | prov_level_sex_idx | 130 | const | 1768 | 100.00 |<br>Using index condition |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+———————–+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’ order by level desc,sex<br>desc;<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+————-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+————-+<br>| 1 | SIMPLE | customer_info | NULL | ref | prov_level_sex_idx | prov_level_sex_idx | 130 | const | 1768 | 100.00 |<br>Using where |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+————-+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>– 索引列顺序和 order by 子句的顺序完全一致 – 所有列的排序方向非全部相同 mysql&gt; explain select name, province, level, sex from customer_info where<br>province &#x3D; ‘zhejiang’ order by level desc,sex;<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+—————————————+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+—————————————+<br>| 1 | SIMPLE | customer_info | NULL | ref | prov_level_sex_idx | prov_level_sex_idx | 130 | const | 1768 | 100.00 |<br>Using index condition; Using filesort |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+—————————————+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’ order by level,sex desc;<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+—————————————+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+—————————————+<br>| 1 | SIMPLE | customer_info | NULL | ref | prov_level_sex_idx | prov_level_sex_idx | 130 | const | 1768 | 100.00 |<br>Using index condition; Using filesort |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+—————————————+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>– 索引列顺序和 order by 子句的顺序非完全一致 – 所有列的排序方向全部相同 mysql&gt; explain select name, province, level, sex from customer_info where<br>province &#x3D; ‘zhejiang’ order by sex,level;<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+—————————————+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+—————————————+<br>| 1 | SIMPLE | customer_info | NULL | ref | prov_level_sex_idx | prov_level_sex_idx | 130 | const | 1768 | 100.00 |<br>Using index condition; Using filesort |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+—————————————+<br>1 row in set, 1 warning (0.00 sec)</p>
<p>mysql&gt; explain select name, province, level, sex from customer_info where province &#x3D; ‘zhejiang’ order by sex desc,level<br>desc;<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+—————————————+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+—————————————+<br>| 1 | SIMPLE | customer_info | NULL | ref | prov_level_sex_idx | prov_level_sex_idx | 130 | const | 1768 | 100.00 |<br>Using index condition; Using filesort |<br>+—-+————-+—————+————+——+——————–+——————–+———+——-+——+———-+—————————————+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>只有当索引的列顺序和 ORDER BY 子句的顺序完全一致，并且所有列的排序方向（倒序或正序）都一样时，mysql 才能够使用索引来对结果做排序。</li>
</ul>
<p>查询条件都在索引中</p>
<p>mysql&gt; explain select province, level, sex from customer_info where province &#x3D; ‘zhejiang’ and level &#x3D; 5 and sex &#x3D; ‘<br>male’;<br>+—-+————-+—————+————+——+——————–+——————–+———+——————-+——+———-+————-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+—————+————+——+——————–+——————–+———+——————-+——+———-+————-+<br>| 1 | SIMPLE | customer_info | NULL | ref | prov_level_sex_idx | prov_level_sex_idx | 176 | const,const,const | 5 |<br>100.00 | Using index |<br>+—-+————-+—————+————+——+——————–+——————–+———+——————-+——+———-+————-+<br>1 row in set, 1 warning (0.00 sec)</p>
<ul>
<li>Extra 字段是 Using index，意思就是索引覆盖，查询的内容可以直接在索引中拿到</li>
</ul>
<p>联表查询</p>
<p>两表为不同的字符集</p>
<p>– create table customer_info – use utf8mb4 CREATE TABLE <code>customer_info</code> (<br><code>id</code> bigint(20) unsigned NOT NULL AUTO_INCREMENT,<br><code>name</code> varchar(32) NOT NULL COMMENT ‘名字’,<br><code>province</code> varchar(32) NOT NULL COMMENT ‘省份’,<br><code>level</code> int(6) NOT NULL COMMENT ‘等级’,<br><code>sex</code> varchar(10) NOT NULL COMMENT ‘性别’, PRIMARY KEY (<code>id</code>)<br>KEY <code>prov</code> (<code>province</code>), KEY <code>lvl_idx</code> (<code>level</code>), KEY <code>nm</code> (<code>name</code>)<br>) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8mb4 ROW_FORMAT&#x3D;DYNAMIC COMMENT&#x3D;’顾客信息表’;</p>
<p>– create table customer_info_2 – use utf8mb4 CREATE TABLE <code>customer_info_2</code> (<br><code>id</code> bigint(20) unsigned NOT NULL AUTO_INCREMENT,<br><code>name</code> varchar(32) NOT NULL COMMENT ‘名字’,<br><code>province</code> varchar(512) NOT NULL COMMENT ‘省份’,<br><code>level</code> int(6) NOT NULL COMMENT ‘等级’,<br><code>sex</code> varchar(10) NOT NULL COMMENT ‘性别’, PRIMARY KEY (<code>id</code>), KEY <code>prov</code> (<code>province</code>), KEY <code>lvl_idx</code> (<code>level</code>),<br>KEY <code>nm</code> (<code>name</code>),<br>) ENGINE&#x3D;InnoDB DEFAULT CHARSET&#x3D;utf8 ROW_FORMAT&#x3D;DYNAMIC COMMENT&#x3D;’顾客信息表’;</p>
<p>查询</p>
<p>mysql&gt; explain select * from customer_info c1 join customer_info_2 c2 on c1.name &#x3D; c2.name where c1.name &#x3D; ‘gdtb’;<br>+—-+————-+——-+————+——+—————+——+———+——-+——-+———-+—————————————————-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+——-+————+——+—————+——+———+——-+——-+———-+—————————————————-+<br>| 1 | SIMPLE | c1 | NULL | ref | nm | nm | 130 | const | 1 | 100.00 | NULL | | 1 | SIMPLE | c2 | NULL | ALL | NULL |<br>NULL | NULL | NULL | 99986 | 100.00 | Using where; Using join buffer (Block Nested Loop) |<br>+—-+————-+——-+————+——+—————+——+———+——-+——-+———-+—————————————————-+<br>2 rows in set, 1 warning (0.00 sec)</p>
<p>mysql&gt; explain select * from customer_info c1 join customer_info_2 c2 on c1.name &#x3D; c2.name where c2.name &#x3D; ‘gdtb’;<br>+—-+————-+——-+————+——+—————+——+———+——-+——+———-+———————–+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+——-+————+——+—————+——+———+——-+——+———-+———————–+<br>| 1 | SIMPLE | c2 | NULL | ref | nm | nm | 98 | const | 1 | 100.00 | NULL | | 1 | SIMPLE | c1 | NULL | ref | nm | nm |<br>130 | func | 1 | 100.00 | Using index condition |<br>+—-+————-+——-+————+——+—————+——+———+——-+——+———-+———————–+<br>2 rows in set, 1 warning (0.00 sec)</p>
<ul>
<li>字符集 utf8mb4 是 utf8 的超集</li>
<li>当 c1 为驱动表时，c1 中过滤出来的 name 字段的字符集为 utf8mb4，然后再去关联 c2 中的 name(utf8) 字段，超集关联子集，这时候 c2 中的 name 字段需要转换为 utf8mb4 字符集，即<br>CONVERT(c2.name USING utf8mb4)，因为在索引列上使用了函数操作，所以索引失效了</li>
<li>当 c2 为驱动表时，c2 中过滤出来的 name 字段的字符集为 utf8，然后再去关联 c1 的 name(utf8mb4) 字段，子集关联超集，这没问题，所以 c1 中的关联字段可以走索引</li>
</ul>
<p>了解了是什么原因造成的，如果要暂时解决这个问题，可以在关联条件中手动将 utf8mb4 向 utf8 进行转换</p>
<p>mysql&gt; explain select * from customer_info c1 join customer_info_2 c2 on CONVERT(c1.name USING utf8) &#x3D; c2.name where<br>c1.name &#x3D; ‘gdtb’;<br>+—-+————-+——-+————+——+—————+——+———+——-+——+———-+———————–+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+——-+————+——+—————+——+———+——-+——+———-+———————–+<br>| 1 | SIMPLE | c1 | NULL | ref | nm | nm | 130 | const | 1 | 100.00 | NULL | | 1 | SIMPLE | c2 | NULL | ref | nm | nm |<br>98 | func | 1 | 100.00 | Using index condition |<br>+—-+————-+——-+————+——+—————+——+———+——-+——+———-+———————–+<br>2 rows in set, 1 warning (0.00 sec)</p>
<p>当然，最好的方法还是让两张表的字符集变得一致。int 类型的不受影响。</p>
<p>索引加在哪个表</p>
<p>mysql&gt; explain select * from customer_info_2 c2 left join customer_info c1 on c2.name &#x3D; c1.name;<br>+—-+————-+——-+————+——+—————+——+———+——+——-+———-+————-+<br>| id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra |<br>+—-+————-+——-+————+——+—————+——+———+——+——-+———-+————-+<br>| 1 | SIMPLE | c2 | NULL | ALL | NULL | NULL | NULL | NULL | 99986 | 100.00 | NULL | | 1 | SIMPLE | c1 | NULL | ref | nm<br>| nm | 130 | func | 1 | 100.00 | Using where |<br>+—-+————-+——-+————+——+—————+——+———+——+——-+———-+————-+<br>2 rows in set, 1 warning (0.00 sec)</p>
<ul>
<li>c1 和 c2 表都有 name 字段的索引，当 c2 为驱动表时，当关联字段不是 where 的条件时，关联字段的索引只需要加在 c1 表(被驱动表)即可，c2 表(驱动表)的字段可以不加。</li>
</ul>
]]></content>
      <categories>
        <category>mysql</category>
        <category>索引</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring-基础</title>
    <url>/2023/02/22/springboot/Spring/</url>
    <content><![CDATA[<h1 id="启动流程"><a href="#启动流程" class="headerlink" title="启动流程"></a>启动流程</h1><p><img src="/img_1.png" alt="img_1.png"></p>
]]></content>
      <categories>
        <category>Spring</category>
        <category>基础</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title>redisCluster</title>
    <url>/2023/03/15/redis/redisCluster/</url>
    <content><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><blockquote>
<p>此篇文章主要介绍分片，主从和哨兵模式会在别的文章中介绍</p>
</blockquote>
<h1 id="一、客户端分片"><a href="#一、客户端分片" class="headerlink" title="一、客户端分片"></a>一、客户端分片</h1><p>在Redis 3.0之前，redis并没有Redis cluster功能，当时大部分使用的是客户端分片以及第三方工具实现分片集群，比如Jedis，<br>Jedis的Redis Sharding实现具有如下特点：  </p>
<ol>
<li>采用一致性哈希算法(consistent hashing)，将key和节点name各自hashing，，然后进行映射匹配，采用的算法是MURMUR_HASH。采用一致性哈希而不是采用简单类似哈希求模映射的主要原因是当增加或减少节点时，不会产生由于重新匹配造成的rehashing。一致性哈希只影响相邻节点key分配，影响量小。</li>
<li>为了避免一致性哈希只影响相邻节点造成节点分配压力，ShardedJedis会对每个Redis节点根据名字(没有，Jedis会赋予缺省名字)会虚拟化出160个虚拟节点进行散列。根据权重weight，也可虚拟化出160倍数的虚拟节点。用虚拟节点做映射匹配，可以在增加或减少Redis节点时，key在各Redis节点移动再分配更均匀，而不是只有相邻节点受影响。(就比如ABC3个节点只能映射出ABC3个散列，如果每个ABC可以虚拟出多N个，即上面说的160个（即现在有3*160个可供给映射），那么存进去的数据则会更加的均匀。)</li>
<li>ShardedJedis支持keyTagPattern模式，即抽取key的一部分keyTag做sharding，这样通过合理命名key，可以将一组相关联的key放入同一个Redis节点，这在避免跨节点访问相关数据时很重要。</li>
</ol>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>客户端自己计算数据的key应该在哪个机器上存储和查找，此方法的好处是降低了服务器集群的复杂度，客户端实现数据分片时，服务器是独立的，服务器之前没有任何关联。多数redis客户端库实现了此功能，也叫sharding,这种方式的缺点是客户端需要实时知道当前集群节点的联系信息，同时，当添加一个新的节点时，客户端要支持动态sharding.，多数客户端实现不支持此功能，需要重启redis。另一个弊端是redis的HA需要额外考虑。</p>
<h1 id="二、代理分片"><a href="#二、代理分片" class="headerlink" title="二、代理分片"></a>二、代理分片</h1><h2 id="twemproxy"><a href="#twemproxy" class="headerlink" title="twemproxy"></a>twemproxy</h2><p>twemproxy处于客户端和服务器的中间，将客户端发来的请求，进行一定的处理后(如sharding)，再转发给后端真正的Redis服务器。也就是说，客户端不直接访问Redis服务器，而是通过twemproxy代理中间件间接访问。</p>
<p>参照Redis Sharding架构，增加代理中间件的Redis集群架构如下：</p>
<ol>
<li>twemproxy中间件的内部处理是无状态的，它本身可以很轻松地集群，这样可避免单点压力或故障。</li>
<li>twemproxy又叫nutcracker，起源于twitter系统中redis&#x2F;memcached集群开发实践，运行效果良好，后代码奉献给开源社区。其轻量高效，采用C语言开发，工程网址是：GitHub - twitter&#x2F;twemproxy: A fast, light-weight proxy for memcached andredis</li>
</ol>
<p>twemproxy后端不仅支持redis，同时也支持memcached，这是twitter系统具体环境造成的。<br>由于使用了中间件，twemproxy可以通过共享与后端系统的连接，降低客户端直接连接后端服务器的连接数量。同时，它也提供sharding功能，支持后端服务器集群水平扩展。统一运维管理也带来了方便。<br>当然，也是由于使用了中间件代理，相比客户端直连服务器方式，性能上会有所损耗</p>
<h1 id="三、服务端分片（redis-cluster）"><a href="#三、服务端分片（redis-cluster）" class="headerlink" title="三、服务端分片（redis cluster）"></a>三、服务端分片（redis cluster）</h1><p>详细介绍可参照：<br><a href="http://www.redis.cn/topics/cluster-tutorial.html">http://www.redis.cn/topics/cluster-tutorial.html</a><br><a href="http://www.redis.cn/topics/cluster-spec.html">http://www.redis.cn/topics/cluster-spec.html</a></p>
<h2 id="槽"><a href="#槽" class="headerlink" title="槽"></a>槽</h2><p>CRC16算法产生的hash值有16bit，该算法可以产生2^16-&#x3D;65536个值；不同节点之间需要相互交换信息，交换的数据信息，由消息体和消息头组成。<br>消息头里面有个myslots的char数组，长度为16383&#x2F;8，这其实是一个bitmap,每一个位代表一个槽，如果该位为1，表示这个槽是属于这个节点的，在消息头中，最占空间的是myslots[CLUSTER_SLOTS&#x2F;8]。这块的大小是:<br>16384÷8÷1024&#x3D;2kb，如果最大使用65536会需要8kb的空间，浪费带宽；</p>
<h2 id="节点通信"><a href="#节点通信" class="headerlink" title="节点通信"></a>节点通信</h2><h2 id="集群不可用"><a href="#集群不可用" class="headerlink" title="集群不可用"></a>集群不可用</h2><h2 id="失效检测和故障转移"><a href="#失效检测和故障转移" class="headerlink" title="失效检测和故障转移"></a>失效检测和故障转移</h2><h2 id="增删节点"><a href="#增删节点" class="headerlink" title="增删节点"></a>增删节点</h2><h2 id="备份迁移"><a href="#备份迁移" class="headerlink" title="备份迁移"></a>备份迁移</h2>]]></content>
      <categories>
        <category>redis</category>
        <category>集群</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
</search>
